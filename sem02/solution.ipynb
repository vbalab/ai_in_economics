{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a10561",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79297679",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "x = torch.ones(N)\n",
    "\n",
    "drop = nn.Dropout(p=0.5)\n",
    "\n",
    "# TRAIN mode: dropout is active (randomly zeros + scales remaining by 1/(1-p))\n",
    "drop.train()\n",
    "y_train1 = drop(x)\n",
    "y_train2 = drop(x)\n",
    "\n",
    "# EVAL mode: dropout is disabled (identity)\n",
    "drop.eval()\n",
    "y_eval = drop(x)\n",
    "\n",
    "print(f\"x        = {x}\\n\")\n",
    "print(f\"train #1 = {y_train1}\")\n",
    "print(f\"train #2 = {y_train2}\\n\")\n",
    "\n",
    "# NOTE: may be not 1.0, but close to it\n",
    "print(f\"train #1 mean = {y_train1.mean().item():.3f}\")\n",
    "print(f\"train #2 mean = {y_train2.mean().item():.3f}\\n\")\n",
    "\n",
    "print(f\"eval     = {y_eval}\")\n",
    "\n",
    "# NOTE: it does PROBABILITY zeroing, but PREDETERMINED scale\n",
    "# -> works good only by LLN (law of large numbers)\n",
    "# -> try larger N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d04ae7",
   "metadata": {},
   "source": [
    "# BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e74c42",
   "metadata": {},
   "source": [
    "$$y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "- not only normalization, but `nn.Linear` within (if `affine=True` - which is default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5283be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHES = 100\n",
    "N = 8\n",
    "C = 2\n",
    "BIAS = 10\n",
    "STD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "train_batches = [torch.randn(N, C) * STD + BIAS for _ in range(BATCHES)]\n",
    "\n",
    "train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65350ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm over 1D features (N, C)\n",
    "bn = nn.BatchNorm1d(C, affine=False)  # keep it pure: no gamma/beta\n",
    "\n",
    "bn(train_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cb6de",
   "metadata": {},
   "source": [
    "But not only current batch normalization is calculated, also running statistics for the **inference (evaluation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[BEFORE] Learned running mean: {bn.running_mean}\")\n",
    "print(f\"[BEFORE] Learned running var : {bn.running_var}\\n\")\n",
    "\n",
    "bn.train()\n",
    "for xb in train_batches:\n",
    "    _ = bn(xb)  # updates running_mean/running_var\n",
    "\n",
    "print(f\"[AFTER] Learned running mean: {bn.running_mean}\")\n",
    "print(f\"[AFTER] Learned running var : {bn.running_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a01e5",
   "metadata": {},
   "source": [
    "BatchNormâ€™s running mean/var approximate the training **data distribution**.  \n",
    "So using them keeps feature scaling consistent with the weights.\n",
    "\n",
    "At **inference** you want the same normalization the model trained against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_BIAS = 20\n",
    "x_test = torch.randn(N, C) * STD + TEST_BATCH_BIAS\n",
    "\n",
    "print(x_test)\n",
    "print(f\"\\nTest batch mean: {x_test.mean(dim=0)}  (should be near {TEST_BATCH_BIAS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d048593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > run this cell several times\n",
    "\n",
    "print(bn(x_test))  # Wrong Inference:\n",
    "\n",
    "print(f\"Learned running mean: {bn.running_mean}\")\n",
    "print(f\"Learned running var : {bn.running_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65134ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > rerun training before this cell\n",
    "\n",
    "bn.eval()\n",
    "y_eval = bn(x_test)  # Correct Inference: uses running stats from training\n",
    "\n",
    "print(y_eval)\n",
    "print(f\"y mean: {y_eval.mean(dim=0)}, y std: {y_eval.std(dim=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65097cc",
   "metadata": {},
   "source": [
    "# Data + Model + Train + Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "lr = 1e-3\n",
    "val_size = 5000\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92532f",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # MNIST mean/std\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
    "\n",
    "train_ds, val_ds = random_split(\n",
    "    train_full,\n",
    "    [len(train_full) - val_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_mnist(x, y, pred=None, n=12):\n",
    "    x_vis = (x.cpu() * 0.3081 + 0.1307).clamp(0, 1)\n",
    "\n",
    "    cols = 6\n",
    "    rows = (n + cols - 1) // cols\n",
    "    plt.figure(figsize=(cols * 2, rows * 2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(x_vis[i, 0], cmap=\"gray\")\n",
    "        prediction = f\", p={pred[i].item()}\" if pred is not None else \"\"\n",
    "        plt.title(f\"y={y[i].item()}{prediction}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "show_mnist(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccf99e",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # why 10? <- logits for each class of [0, 1, ..., 9]\n",
    "        self.fc3 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ClassifierMLP().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0b89b",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65db92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss_and_acc(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        n += y.size(0)\n",
    "    return total_loss / n, correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31456fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        running += loss.item() * y.size(0)\n",
    "        n += y.size(0)\n",
    "\n",
    "    train_loss = running / n\n",
    "    val_loss, val_acc = eval_loss_and_acc(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    clear_output()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(epoch + 1), train_losses, label=\"train\")\n",
    "    plt.plot(range(epoch + 1), val_losses, label=\"val\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"MNIST MLP: train vs val loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c699d59",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = eval_loss_and_acc(test_loader)\n",
    "print(f\"TEST | loss={test_loss:.4f} | acc={test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x, y = next(iter(test_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "logits = model(x)\n",
    "pred = logits.argmax(dim=1)\n",
    "\n",
    "show_mnist(x, y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee8797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
