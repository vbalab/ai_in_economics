{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51e214e",
   "metadata": {},
   "source": [
    "# Seminar 03 — Residual Blocks (Very Detailed)\n",
    "\n",
    "A **residual block** is a neural network building block that learns a *residual function* — i.e., the difference between the desired output and the input — instead of learning the full transformation directly. Formally, if a block receives an input tensor **x**, the block learns a function **F(x)** and returns:\n",
    "\n",
    "\\[\text{output} = x + F(x)\\]\n",
    "\n",
    "This idea was popularized in **ResNet** (He et al., 2015). Residual connections are now a standard component in modern deep learning architectures (vision, NLP, diffusion models, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What is a residual block?\n",
    "\n",
    "A residual block is a small sub-network with a **skip connection** (also called a shortcut or identity connection). Instead of outputting only `F(x)`, it outputs `x + F(x)` (or a projection `W_s x + F(x)` when dimensions differ).\n",
    "\n",
    "### Intuition\n",
    "* If learning the full mapping is hard, learning the *difference* from the identity can be easier.\n",
    "* If the best solution is close to identity, the network can push `F(x)` toward **0**, so the block approximates the identity `x`.\n",
    "* This eases optimization and reduces the risk of vanishing gradients in deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Why do we need residual blocks?\n",
    "\n",
    "### 2.1) Optimization gets hard as depth grows\n",
    "Deep networks can be difficult to train due to **vanishing/exploding gradients**. As the number of layers increases, the signal that backpropagates to early layers can become too small. Residual connections create *direct gradient paths* from deeper layers to earlier ones.\n",
    "\n",
    "### 2.2) Identity is a strong baseline\n",
    "If a deeper network is not helpful, it should at least behave like a shallower one. With residual blocks, the network can easily learn the identity mapping by driving `F(x) → 0`. Without residual connections, an identity mapping might require carefully tuned weights.\n",
    "\n",
    "### 2.3) Empirical gains\n",
    "Residual blocks:\n",
    "* Enable **much deeper networks**.\n",
    "* Improve **accuracy** and **generalization**.\n",
    "* Make training more stable, especially with normalization layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) How do we implement a residual block?\n",
    "\n",
    "There are multiple standard designs. Below are the most common patterns and why you might choose them.\n",
    "\n",
    "### 3.1) Basic residual block (same shape)\n",
    "If input and output shapes are identical, use a simple **identity skip**:\n",
    "\n",
    "\\[\text{output} = x + F(x)\\]\n",
    "\n",
    "Example structure:\n",
    "```\n",
    "Conv -> ReLU -> Conv -> (Add x) -> ReLU\n",
    "```\n",
    "\n",
    "### 3.2) Projection residual block (different shape)\n",
    "If the block changes the number of channels or spatial resolution, the skip path uses a **projection** (e.g., 1x1 convolution or linear layer) so shapes match:\n",
    "\n",
    "\\[\text{output} = W_s x + F(x)\\]\n",
    "\n",
    "### 3.3) Pre-activation residual block\n",
    "In the original ResNet v2, normalization + activation is applied **before** the main layers:\n",
    "```\n",
    "BN -> ReLU -> Conv -> BN -> ReLU -> Conv -> (Add x)\n",
    "```\n",
    "This can improve gradient flow and stability.\n",
    "\n",
    "### 3.4) Bottleneck residual block\n",
    "Used in deeper ResNets (e.g., ResNet-50/101). The idea is to reduce computation:\n",
    "```\n",
    "1x1 (reduce channels) -> 3x3 -> 1x1 (restore channels)\n",
    "```\n",
    "\n",
    "### 3.5) Residual block variants\n",
    "Residual connections appear in many architectures:\n",
    "* **Transformer**: `x + Attention(x)` and `x + MLP(x)`.\n",
    "* **U-Net**: residual blocks in encoder/decoder.\n",
    "* **Diffusion models**: residual blocks with time embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) How do we use residual blocks?\n",
    "\n",
    "You use residual blocks as reusable building blocks to assemble deeper networks. A model can be:\n",
    "\n",
    "```\n",
    "Input -> ResidualBlock -> ResidualBlock -> ... -> Head -> Output\n",
    "```\n",
    "\n",
    "In practice:\n",
    "* Use **identity skips** when input/output shapes match.\n",
    "* Use **projection skips** when you change feature dimensions.\n",
    "* Stack multiple blocks to increase depth without losing trainability.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Python 3.14 Example Code (Framework-agnostic + PyTorch-style)\n",
    "\n",
    "Below is a fully self-contained example of a residual block using **NumPy**, followed by a PyTorch-style implementation. Both follow the same logic and illustrate how you would actually use a residual block in a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.14 - Residual Block Example (NumPy)\n",
    "# This is a minimal, educational example with simple linear layers.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "\n",
    "Array = np.ndarray\n",
    "\n",
    "\n",
    "def relu(x: Array) -> Array:\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Linear:\n",
    "    weight: Array\n",
    "    bias: Array\n",
    "\n",
    "    @classmethod\n",
    "    def init(cls, in_features: int, out_features: int, rng: np.random.Generator) -> \"Linear\":\n",
    "        # He initialization for ReLU\n",
    "        scale = np.sqrt(2.0 / in_features)\n",
    "        weight = rng.normal(0.0, scale, size=(in_features, out_features))\n",
    "        bias = np.zeros((out_features,), dtype=np.float64)\n",
    "        return cls(weight=weight, bias=bias)\n",
    "\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResidualBlock:\n",
    "    # Two-layer residual block with optional projection for the skip path\n",
    "    layer1: Linear\n",
    "    layer2: Linear\n",
    "    activation: Callable[[Array], Array] = relu\n",
    "    projection: Linear | None = None\n",
    "\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        # Main path\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        # Skip path (identity or projection)\n",
    "        skip = x if self.projection is None else self.projection(x)\n",
    "\n",
    "        # Residual addition + final activation\n",
    "        return self.activation(out + skip)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Case 1: Identity skip (same feature size)\n",
    "block_same = ResidualBlock(\n",
    "    layer1=Linear.init(8, 8, rng),\n",
    "    layer2=Linear.init(8, 8, rng),\n",
    ")\n",
    "\n",
    "x = rng.normal(size=(4, 8))  # batch=4, features=8\n",
    "out_same = block_same(x)\n",
    "print(\"Identity-skip output shape:\", out_same.shape)\n",
    "\n",
    "# Case 2: Projection skip (different feature size)\n",
    "block_proj = ResidualBlock(\n",
    "    layer1=Linear.init(8, 16, rng),\n",
    "    layer2=Linear.init(16, 16, rng),\n",
    "    projection=Linear.init(8, 16, rng),\n",
    ")\n",
    "\n",
    "out_proj = block_proj(x)\n",
    "print(\"Projection-skip output shape:\", out_proj.shape)\n",
    "\n",
    "# Stack blocks into a tiny model\n",
    "blocks = [\n",
    "    block_same,\n",
    "    ResidualBlock(\n",
    "        layer1=Linear.init(8, 8, rng),\n",
    "        layer2=Linear.init(8, 8, rng),\n",
    "    ),\n",
    "]\n",
    "\n",
    "h = x\n",
    "for b in blocks:\n",
    "    h = b(h)\n",
    "print(\"Stacked output shape:\", h.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: PyTorch-style residual block (if torch is available)\n",
    "# This mirrors real-world practice but keeps the example simple.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class ResidualBlockTorch(nn.Module):\n",
    "        def __init__(self, in_features: int, out_features: int):\n",
    "            super().__init__()\n",
    "            self.layer1 = nn.Linear(in_features, out_features)\n",
    "            self.layer2 = nn.Linear(out_features, out_features)\n",
    "            self.act = nn.ReLU()\n",
    "            self.projection = None\n",
    "            if in_features != out_features:\n",
    "                self.projection = nn.Linear(in_features, out_features)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.layer1(x)\n",
    "            out = self.act(out)\n",
    "            out = self.layer2(out)\n",
    "            skip = x if self.projection is None else self.projection(x)\n",
    "            return self.act(out + skip)\n",
    "\n",
    "    # Example usage\n",
    "    x_t = torch.randn(4, 8)\n",
    "    block_t = ResidualBlockTorch(8, 8)\n",
    "    y_t = block_t(x_t)\n",
    "    print(\"Torch output shape:\", y_t.shape)\n",
    "except ModuleNotFoundError:\n",
    "    print(\"PyTorch not installed; skipping torch example.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
