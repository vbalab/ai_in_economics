{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28847d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from torch.nn import functional as F\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353fc0d",
   "metadata": {},
   "source": [
    "# I. Prerequisits\n",
    "\n",
    "## BPE (Byte Pair Encoding) Tokenization\n",
    "\n",
    "1. Initialize vocabulary: all individual characters (or bytes).\n",
    "\n",
    "2. Count frequencies of all adjacent symbol pairs.\n",
    "\n",
    "3. Merge the most frequent pair into a new symbol.\n",
    "\n",
    "4. Repeat steps 2–3 until reaching the desired vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c80a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 2000\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train(files=[\"train.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88c8043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hel', 'l', 'o', 'words', 'tokenization']\n",
      "[78, 15, 18, 59, 136]\n"
     ]
    }
   ],
   "source": [
    "# 3) Use it\n",
    "enc = tokenizer.encode(\"hello words tokenization\")\n",
    "print(enc.tokens)\n",
    "print(enc.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8aa21",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "`nn.Embedding` - **trainable** lookup table that maps token IDs to dense vectors.\n",
    "\n",
    "It is initialized with random weights (_Xavier_) and learns them during training.\n",
    "\n",
    "Text is tokenized $\\to$ converted to IDs $\\to$ passed through nn.Embedding $\\to$ turned into embeddings for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c02070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3215, -1.5927,  0.7108,  ...,  2.0676,  0.0083,  0.4260],\n",
       "        [-1.7744,  0.1643, -0.4902,  ...,  1.4040,  0.2221,  0.2700],\n",
       "        [-0.5170, -0.4645,  0.6339,  ..., -0.1314,  0.2753,  0.8073],\n",
       "        ...,\n",
       "        [-2.6290,  0.5621, -0.4083,  ..., -0.4444,  1.5147, -0.2869],\n",
       "        [ 1.9268,  0.6434, -1.1821,  ...,  0.0537,  1.0854,  0.1743],\n",
       "        [-0.0758, -1.6519,  2.5185,  ..., -0.7546,  1.6106, -0.4367]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = VOCAB_SIZE\n",
    "embedding_dim = 512\n",
    "\n",
    "emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "emb.weight      # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334adf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5533,  0.1166, -0.4037,  ...,  0.4473,  1.9527,  2.4356],\n",
       "        [ 1.0676,  0.7293,  0.0139,  ..., -0.5089,  0.1529,  1.7803],\n",
       "        [ 0.1888,  0.6157,  0.2174,  ...,  0.2663,  1.2283, -1.0712],\n",
       "        [-0.3183, -0.7785, -2.7941,  ...,  0.9561,  1.8599,  1.2893],\n",
       "        [ 0.7906,  0.4859,  1.5388,  ..., -1.2838, -1.1070, -0.8138]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.tensor([78, 15, 18, 59, 136])\n",
    "out = emb(token_ids)\n",
    "\n",
    "out             # shape: (3, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb93dc",
   "metadata": {},
   "source": [
    "# II. Transformer\n",
    "\n",
    "## 1. Vanilla Encoder–Decoder (Sutskever et al., 2014)\n",
    "\n",
    "**Architecture:** RNN encoder $\\to$ fixed vector $\\to$ RNN decoder.  \n",
    "No attention.\n",
    "\n",
    "### Math\n",
    "\n",
    "- Encoder reads inputs $ x_1, \\dots, x_T $:\n",
    "\n",
    "$$\n",
    "h_t = \\mathrm{RNN}_{\\text{enc}}(h_{t-1}, x_t), \\quad h_0 = 0\n",
    "$$\n",
    "\n",
    "- Final hidden state $ h_T $ is the **context vector** $ c $.\n",
    "\n",
    "- Decoder generates outputs sequentially:\n",
    "\n",
    "$$\n",
    "s_t = \\mathrm{RNN}_{\\text{dec}}(s_{t-1}, y_{t-1}, c)\n",
    "$$\n",
    "$$\n",
    "P(y_t \\mid y_{<t}, x) = \\mathrm{Softmax}(W_o s_t + b_o)\n",
    "$$\n",
    "\n",
    "## 2. Bahdanau (Additive) Attention (Bahdanau et al., 2014)\n",
    "\n",
    "**Architecture:** RNN encoder–decoder with additive attention.  \n",
    "Attention is computed **before** the decoder RNN step (_pre-RNN attention_).\n",
    "\n",
    "## 3. Luong (Multiplicative) Attention (Luong et al., 2015)\n",
    "\n",
    "**Architecture:** RNN encoder–decoder with multiplicative attention.  \n",
    "Attention is computed **after** the decoder RNN step (_post-RNN attention_).  \n",
    "\n",
    "## 4. Transformer Attention ([Attention Is All You Need [2017]](https://arxiv.org/abs/1706.03762))\n",
    "\n",
    "**Architecture:** Encoder–decoder model without any RNN / LSTM / CNN / ..., do only attention.\n",
    "\n",
    "So 2. and 3. did attention only _between_ encoder & decoder (that were RNN / ...), now encoder & decoder are attention themselves (and we also keep doing attention between).\n",
    "\n",
    "### Step 1: Input Representations\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{n_{sequence} \\times d_{\\text{embedding}}}\n",
    "$$\n",
    "where row $X_i$ is the embedding of token $i$.  \n",
    "\n",
    "### Step 2: $Q, K, V$\n",
    "\n",
    "The Transformer learns **three linear projection matrices per head**:\n",
    "\n",
    "$$\n",
    "W^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad\n",
    "W^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad\n",
    "W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "Q = X W^Q, \\quad K = X W^K, \\quad V = X W^V\n",
    "$$\n",
    "\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$ (queries) - \"What am I looking for?\"\n",
    "\n",
    "- $K \\in \\mathbb{R}^{n \\times d_k}$ (keys) - \"What properties do I have?\"\n",
    "\n",
    "- $V \\in \\mathbb{R}^{n \\times d_v}$ (values) - \"What information should I pass on if selected?\"\n",
    "\n",
    "### Step 3: Scoring Queries Against Keys\n",
    "\n",
    "How much should token $i$ pay attention to token $j$?\n",
    "\n",
    "The raw **attention score** between query $Q_i$ and key $K_j$ is:\n",
    "\n",
    "$$\n",
    "\\text{score}(i,j) = Q_i \\cdot K_j^T = \\sum_{m=1}^{d_k} Q_{i,m} K_{j,m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Scores} = Q K^T \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "\n",
    "Row $i$ = how much token $i$ attends to all tokens.\n",
    "\n",
    "### Step 4: Scaling by $\\sqrt{d_k}$\n",
    "\n",
    "#### **Assumptions**\n",
    "\n",
    "1. $$\n",
    "    \\mathbb{E}[k_i] = \\mathbb{E}[q_i] = 0, \\quad \\mathrm{Var}[k_i] = \\mathrm{Var}[q_i] = 1\n",
    "    $$\n",
    "\n",
    "    - Under standard weight initialization (e.g., Xavier) that is okay\n",
    "\n",
    "2. $ k_i, q_i $ are _independent_ across dimensions\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}[K^\\top Q]\n",
    "= \\sum_{i=1}^{d_k} \\mathrm{Var}[k_i q_i]\n",
    "= \\sum_{i=1}^{d_k} \\mathrm{Var}[k_i] \\mathrm{Var}[q_i]\n",
    "= d_k\n",
    "$$\n",
    "\n",
    "#### **Scaling**\n",
    "\n",
    "Because large values push softmax into saturation, leading to vanishing gradients, we scale by $\\sqrt{d_k}$ (like temperature):\n",
    "\n",
    "$$\n",
    "\\text{ScaledScores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "### Step 5: Attention\n",
    "\n",
    "**Attention weights (scores)**:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "\n",
    "- Row $i$: distribution over which tokens $i$ attends to.  \n",
    "- Each row sums to 1.  \n",
    "\n",
    "**Attention output (pattern)**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = AV\n",
    "$$\n",
    "\n",
    "### Step 6: Multi-Head Attention\n",
    "\n",
    "> _multi_-head attention instead of a _single_ head to focus on different representation subspaces: Syntactic structure, Positional patterns, Semantic roles, ...\n",
    "\n",
    "For $h$ heads, we repeat the above with different projection matrices:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(X W_i^Q, \\, X W_i^K, \\, X W_i^V)\n",
    "$$\n",
    "\n",
    "Then concatenate:\n",
    "\n",
    "$$\n",
    "\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "with $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
    "\n",
    "\n",
    "## Transformer Architecture Overview\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"transformer.png\" alt=\"transformer\" />\n",
    "</p>\n",
    "\n",
    "- **Encoder** (left block): processes the input sequence into a contextual representation.\n",
    "\n",
    "- **Decoder** (right block): generates the output sequence one token at a time.\n",
    "\n",
    "  Attends both to previously generated tokens and the encoder’s representation:\n",
    "\n",
    "  - Queries $\\{Q_i\\}$ come from the decoder.\n",
    "  - Keys $\\{K_i\\}$ & Values $\\{V_i\\}$ come from encoder output.\n",
    "\n",
    "  Allows decoder to focus on relevant input parts when generating output.\n",
    "\n",
    "> Embeddings flow from the last layer of Encoder block into all Decoder block's attention (not from the respective (соответствующих) layers).\n",
    "\n",
    "### Masked Multi-Head Attention\n",
    "\n",
    "The mask is just 0&1 matrix for decoder not to look into future.\n",
    "\n",
    "## BERT-GPT-BART\n",
    "\n",
    "1. BERT (encoder-only)\n",
    "\n",
    "  Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "2. GPT-x (decoder-only)\n",
    "\n",
    "  Generative Pretrained Transformer.\n",
    "\n",
    "3. BART (encoder-decoder)\n",
    "\n",
    "  Encoder reads corrupted text in full (like BERT).\n",
    "\n",
    "  Decoder generates clean text autoregressively (like GPT).\n",
    "\n",
    "### Why GPT-x, but not BART?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2cbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttentionLayer(nn.Module):\n",
    "    def __init__(self, enc_size, dec_size, hid_size):\n",
    "        \"\"\"\n",
    "        Scaled dot-product attention with projections\n",
    "        :param enc_size: num units in encoder state\n",
    "        :param dec_size: num units in decoder state\n",
    "        :param hid_size: dimension of query/key space (d_k)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Define projection layers\n",
    "        # W_q, W_k, W_v project decoder states into query, key, value\n",
    "        self.wq = nn.Linear(dec_size, hid_size, bias=False)\n",
    "        self.wk = nn.Linear(enc_size, hid_size, bias=False)\n",
    "        self.wv = nn.Linear(enc_size, hid_size, bias=False)\n",
    "\n",
    "    def forward(self, enc, dec, input_mask):\n",
    "        \"\"\"\n",
    "        # B - batch_size\n",
    "        # T - seq_len\n",
    "        # H - hid_dim\n",
    "\n",
    "        Compute attention response and weights\n",
    "        :param enc: encoder sequence [B, T, enc_size]\n",
    "        :param dec: decoder state [B, dec_size]\n",
    "        :param input_mask: mask [B, T] (0 = padding)\n",
    "        :returns: attn [B, H], probs [B, T]\n",
    "        \"\"\"\n",
    "\n",
    "        q = self.wq(dec).unsqueeze(1)                           # [B, 1, H]; unsqueeze, so we could multiply with k.T\n",
    "        k = self.wk(enc)                                        # [B, T, H]\n",
    "        v = self.wv(enc)                                        # [B, T, H]\n",
    "\n",
    "        logits = torch.bmm(q, k.transpose(1, 2)).squeeze(1)     # [B, T]\n",
    "        scaled_logits = logits / math.sqrt(self.hid_size)\n",
    "        masked_logits = scaled_logits.masked_fill(~input_mask, float('-inf'))\n",
    "\n",
    "        probs = torch.softmax(masked_logits, dim=-1)            # [B, T]\n",
    "\n",
    "        attention = torch.bmm(probs.unsqueeze(1), v).squeeze(1) # [B, H]\n",
    "\n",
    "        return attention, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6e322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal GPT-style (decoder-only) Transformer using torch.nn building blocks.\n",
    "    - Token + positional embeddings\n",
    "    - Stack of TransformerEncoderLayer with a causal (future-masking) attention mask\n",
    "    - LM head tied to token embeddings (optional but typical for GPT-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        n_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        tie_weights: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by n_heads\")\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # EncoderLayer works fine for GPT-style decoder-only if we apply a causal mask.\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,   # (B, T, C)\n",
    "            norm_first=True,    # closer to many GPT variants\n",
    "        )\n",
    "        self.blocks = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        if tie_weights:\n",
    "            self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        # Precompute a boolean causal mask for max_seq_len.\n",
    "        # True = masked out (disallowed attention) for Transformer modules when using bool masks.\n",
    "        causal = torch.triu(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool), diagonal=1)\n",
    "        self.register_buffer(\"causal_mask\", causal, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,             # (B, T) token ids\n",
    "        targets: torch.Tensor | None = None,  # (B, T) token ids\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          logits: (B, T, vocab_size)\n",
    "          loss:   scalar or None\n",
    "        \"\"\"\n",
    "        if idx.dim() != 2:\n",
    "            raise ValueError(\"idx must have shape (B, T)\")\n",
    "        bsz, T = idx.shape\n",
    "        if T > self.max_seq_len:\n",
    "            raise ValueError(f\"Sequence length {T} exceeds max_seq_len={self.max_seq_len}\")\n",
    "\n",
    "        device = idx.device\n",
    "        pos = torch.arange(T, device=device)  # (T,)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # Slice causal mask to current sequence length.\n",
    "        attn_mask = self.causal_mask[:T, :T]  # (T, T), bool\n",
    "\n",
    "        x = self.blocks(x, mask=attn_mask)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.Tensor,          # (B, T)\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simple autoregressive sampling.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            idx_cond = idx[:, -self.max_seq_len :]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # (B, V)\n",
    "\n",
    "            if temperature <= 0:\n",
    "                # greedy\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                logits = logits / temperature\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
    "                    cutoff = v[:, -1].unsqueeze(-1)\n",
    "                    logits = torch.where(logits < cutoff, torch.full_like(logits, -float(\"inf\")), logits)\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_3_14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
