{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "notebook_dir = \"/home/balabaevvl/nes/ai_economics/sem01\"  # notebook's dir\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "GPUs = [\n",
    "    \"GPU-e83bd31b-fcb9-b8de-f617-2d717619413b\",\n",
    "    \"GPU-5a9b7750-9f85-49a5-3aae-fe07b1b7661d\",\n",
    "    \"GPU-fe2d8dfd-06f2-a5c4-a7fd-4a5f23947005\",\n",
    "    \"GPU-0c320096-21ee-4060-8731-826ca2febfab\",\n",
    "    \"GPU-baef952c-6609-aace-3b78-e4e07788d5de\",\n",
    "    \"GPU-3979d65b-c238-4e9c-0c1c-1aa3f05c56a1\",\n",
    "    \"GPU-6c76a2c5-5375-aa06-11d4-0fddfac30e91\",\n",
    "]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPUs[2]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение нейросетей: NumPy, PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный ноутбук посвещен изучению функционала и абстракций, которые нам представляет PyTorch для разработки нейронных сетей. Основные моменты, которые будут рассматриваться на данном семинаре:\n",
    "* Реализация оптимизации градиентным спуском, используя NumPy\n",
    "* Использование PyTorch Tensors, как замены NumPy ndarrays\n",
    "* Автоматизация подсчета градиентов при помощи `torch.autograd`\n",
    "* Использование `torch.nn` для построения вычислительного графа при помощи уже реализованных слоев\n",
    "* Пакет `torch.optim`, предоставляющий абтракцию оптимизации нейросетей и обновления ее весов\n",
    "* Использование модулей `Dataset`, `DataLoader` из `torch.utils.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2020  100  2020    0     0  17494      0 --:--:-- --:--:-- --:--:-- 17565\n",
      "Archive:  test.zip\n",
      "  inflating: test/inp.npy            \n",
      "  inflating: test/inp_grad.npy       \n",
      "  inflating: test/out.npy            \n"
     ]
    }
   ],
   "source": [
    "!curl -O 'https://courses.cv-gml.ru/storage/seminars/nn-training-numpy-pytorch/test.zip'\n",
    "!unzip -o test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Increase these if figures appear small\n",
    "plt.rcParams[\"figure.figsize\"] = fx, fy = (14.08, 6.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_data(num_samples, num_features, num_targets, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    feature_locs = rng.normal(size=(1, num_features))\n",
    "    feature_scales = np.exp(rng.normal(size=(1, num_features)))\n",
    "    features = rng.normal(\n",
    "        loc=feature_locs,\n",
    "        scale=feature_scales,\n",
    "        size=(num_samples, num_features),\n",
    "    )\n",
    "\n",
    "    targets = []\n",
    "    for _ in range(num_targets):\n",
    "        num_deps = 1 + rng.integers(num_features)\n",
    "        dep_inds = rng.choice(num_features, num_deps)\n",
    "        deps = features[:, dep_inds]\n",
    "\n",
    "        offsets = rng.uniform(0, 2 * np.pi, size=num_deps)\n",
    "        scales = rng.normal(size=num_deps)\n",
    "\n",
    "        target = scales[None, :] * np.sin(deps + offsets[None, :])\n",
    "        targets.append(target.sum(axis=-1))\n",
    "\n",
    "    targets = np.stack(targets, axis=-1)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results():\n",
    "    # Show training loss history\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Training loss history\")\n",
    "    plt.plot(loss_hist)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "    # Visualize ground truth vs predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Ground truth vs predictions\")\n",
    "    for i in range(D_out):\n",
    "        plt.scatter(y_vis[:, i], p_vis[:, i], s=1)\n",
    "    plt.xlabel(\"y\")\n",
    "    plt.ylabel(\"p\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разминка на NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как переходить к рассмотрению PyTorch, давайте сначала реализуем логику для запуска и оптимизации градиентным спуском простой нейросети с помощью NumPy.\n",
    "\n",
    "NumPy - это framework для научных вычислений; он ничего не знает о вычислительных графах, глубоком обучении или градиентах. Однако мы можем легко использовать NumPy для обучения простой двухслойной нейросети на случайных данных вручную, реализуя прямые и обратные проходы через сеть с помощью NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is number of samples; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 100, 16, 100, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input and output data\n",
    "x, y = create_random_data(N, D_in, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "scale1 = 1 / np.sqrt(D_in)\n",
    "scale2 = 1 / np.sqrt(H)\n",
    "w1 = np.random.uniform(-scale1, scale1, size=(D_in, H))\n",
    "w2 = np.random.uniform(-scale2, scale2, size=(H, D_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x @ w1).clip(0, None) @ w2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the train loop\n",
    "learning_rate = 1e-4\n",
    "num_steps = 50_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{2}}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_2}{\\partial W^{2}}\\Big)^{\\!\\top}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "=\n",
    "a_1^{\\top}(h_2-y),\n",
    "\\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "=\n",
    "\\frac{\\partial}{\\partial h_2}\\Big(\\tfrac{1}{2}\\|h_2-y\\|_F^2\\Big)\n",
    "=\n",
    "h_2 - y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{1}}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\\frac{\\partial \\mathcal{L}}{\\partial h_1}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\n",
    "\\Bigg[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1}\n",
    "\\ \\odot\\\n",
    "\\frac{\\partial a_1}{\\partial h_1}\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\n",
    "\\Bigg[\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "\\Big(\\frac{\\partial h_2}{\\partial a_1}\\Big)^{\\!\\top}\n",
    "\\ \\odot\\\n",
    "\\frac{\\partial a_1}{\\partial h_1}\n",
    "\\Bigg]\n",
    "=\n",
    "x^{\\top}\\Big(((h_2-y)(W^{2})^{\\top})\\odot(h_1> 0)\\Big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee080b2dad2f47b0a6cbcca9734f1b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "\n",
    "for t in progress:\n",
    "    h_1 = x @ w1                # [N, D_in] @ [D_in, H] -> [N, H]\n",
    "    a_1 = h_1.clip(0, None)     # [N, H]\n",
    "    h_2 = a_1 @ w2              # [N, H] @ [H, D_out] -> [N, D_out]\n",
    "\n",
    "    loss = (1 / 2) * ((h_2 - y) ** 2).sum() / N\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(loss)\n",
    "    progress.desc = f\"loss: {loss:.8f}\"\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    delta = (h_2 - y) / N                           # [N, D_out]\n",
    "    grad_w2 = a_1.T @ delta                         # [H, N] @ [N, D_out] = [H, D_out] - same as w2\n",
    "    grad_w1 = x.T @ ((delta @ w2.T) * (h_1 >= 0))   # [D_in, N] ([N, D_out] @ [D_out, H]) * [N, H] = [D_in, H] - same as w1\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1408x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement inference/prediction\n",
    "p = (x @ w1).clip(0, None) @ w2\n",
    "\n",
    "p_vis = p\n",
    "y_vis = y\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy - отличный фреймворк, но он не может использовать GPU для ускорения численных вычислений. Для современных нейронных сетей графические процессоры часто обеспечивают ускорение в 50 раз или больше, поэтому использование NumPy не достаточно для реализации современных методов глубинного обучения.\n",
    "\n",
    "Здесь мы представляем самую фундаментальную концепцию PyTorch - тензоры. Тензоры в PyTorch - концептуально идентичны N-мерным массивам NumPy. PyTorch предоставляет множество функций для работы с этими тензорами. Любые вычисления, которые вы, возможно, захотите выполнить с помощью NumPy, также могут быть выполнены с помощью тензоров PyTorch. Многие (**но не все**) функции и методы для работы с тензорами имеют те же имена и интерфейсы как в NumPy.\n",
    "\n",
    "В отличие от N-мерных массивов NumPy, тензоры PyTorch могут использовать GPU для ускорения численных вычислений. Чтобы создать тензор на GPU, можно передать соответствующий аргумент `device` при создании этого тензора. Уже существующий тензор можно скопировать на нужный девайс, используя методы `tensor.to(device)`, `tensor.cpu()` или `tensor.cuda()`.\n",
    "\n",
    "Для конвертации между существующими N-мерными NumPy массивами и PyTorch тензорами, можно использовать функцию `torch.from_numpy(ndarray)` и метод `tensor.numpy()`. Обратите внимание, что для конвертации PyTorch тензора в NumPy массив необходимо, чтобы этот тензор находился на CPU.\n",
    "\n",
    "В данном разделе мы используем тензоры PyTorch, чтобы обучить нейросеть на случайных данных. Как и в приведенном выше примере на NumPy, давайте вручную реализуем прямой и обратный проходы по сети, используя операции с тензорами PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using the GPU 😊\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using the CPU 😞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_random_data(N, D_in, D_out)\n",
    "\n",
    "# Load input and output data onto the device\n",
    "x = torch.from_numpy(x).to(dtype=torch.float32, device=device)\n",
    "y = torch.from_numpy(y).to(dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = torch.empty((D_in, H), device=device)\n",
    "w2 = torch.empty((H, D_out), device=device)\n",
    "w1.uniform_(-scale1, scale1)\n",
    "w2.uniform_(-scale2, scale2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "\n",
    "for t in progress:\n",
    "    h_1 = x @ w1                        # [N, D_in] @ [D_in, H] -> [N, H]\n",
    "    a_1 = h_1.clip(0, None)             # [N, H]\n",
    "    h_2 = a_1 @ w2                      # [N, H] @ [H, D_out] -> [N, D_out]\n",
    "\n",
    "    loss = (1 / 2) * ((h_2 - y) ** 2).sum() / N\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(loss.item())\n",
    "    progress.desc = f\"loss: {loss:.8f}\"\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    delta = (h_2 - y) / N               # [N, D_out]\n",
    "    \n",
    "    rest = delta                        # [N, D_out]\n",
    "    grad_w2 = a_1.T @ rest              # [H, N] @ [N, D_out] = [H, D_out] - same as w2\n",
    "    \n",
    "    rest = (delta @ w2.T) * (h_1 >= 0)   # [N, D_out] @ [D_out, H]) * [N, H] = [N, H]\n",
    "    grad_w1 = x.T @ rest                # [D_in, N] @ [N, H] = [D_in, H] - same as w1\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement inference/prediction\n",
    "p = (x @ w1).clip(0, None) @ w2\n",
    "\n",
    "p_vis = p.cpu().numpy()\n",
    "y_vis = y.cpu().numpy()\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере выше нами был реализован, как прямой проход по сети, так и обратное распространение ошибки. Реализовывать каждый раз обратное распространение ошибки неэффективно и громоздко, ведь как мы знаем современные нейросетевые архитектуры могут состоять из сотен слоев и содержать тысячи операций в самых разных нетривиальных конфигурациях.\n",
    "\n",
    "Для этого в PyTorch реализована поддержка [автоматического дифференциирования в обратном режиме](https://en.wikipedia.org/wiki/Automatic_differentiation) (aka backprop). При использовании `torch.autograd`, выполнение прямого прохода вашей сети будет автоматически сохранять необходимые промежуточные результаты и записывать граф вычислительных зависимостей. Узлы в этом графе будут тензорами, а ребра - функциями, которые производят выходные тензоры из входных тензоров. Обратное распространение ошибки через этот граф затем позволяет легко вычислять градиенты.\n",
    "\n",
    "На практике использование данной технологии весьма простое: если мы хотим вычислить градиенты относительно некоторого тензора, тогда мы устанавливаем атрибут `requires_grad = True` при создании этого тензора или вызвав метод `tensor.requires_grad_()`. Любые дифференциируемые операции PyTorch с этим тензором будут автоматически возвращать тензоры, которые будут \"помнить\" свой вычислительный граф, что позволит нам позже выполнить обратное распространение ошибки через них. Если `x` - тензор с `requires_grad = True`, то после обратного распространения ошибки, `x.grad` будет другим тензором, содержащим градиент `x` относительно некоторого скалярного значения.\n",
    "\n",
    "Если нам наоборот не хочется сторить граф для выполнения обратного распространения ошибки через тензор с установленным атрибутом `requires_grad = True` (например, когда мы обновляем веса модели), то мы можем использовать контекстный менеджер `torch.no_grad()`, чтобы избежать построения вычислительного графа. В ситуации, когда у нас уже есть тензор `x`, являющийся частью графа, то можно получить копию данных этого тензора \"отщепленную\" от графа зависимостей выполнив операцию `x.detach()`.\n",
    "\n",
    "Ниже мы реализуем обучние нейронной сети с использованием автоматического подсчета градиентов `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.empty((D_in, H), device=device, requires_grad=True)\n",
    "w2 = torch.empty((H, D_out), device=device, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    w1.uniform_(-scale1, scale1)\n",
    "    w2.uniform_(-scale2, scale2)\n",
    "\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    p = (x @ w1).clamp_min(0) @ w2\n",
    "    # p = (x @ w1).relu() @ w2\n",
    "\n",
    "    # Compute the loss. Loss must be a scalar tensor\n",
    "    loss = (1 / 2) * ((p - y) ** 2).sum() / N\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    # CAUTION! Don't forget that the loss tensor lives on the device\n",
    "    # and keeps a reference to the whole computation graph!\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(loss.item())\n",
    "    progress.desc = f\"loss: {loss:.8f}\"\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # CAUTION! Autograd loss.backward() ACCUMULATES the gradients, it effectively\n",
    "    # does `w.grad += ...` for each tensor with requires_grad=True in the graph.\n",
    "    # You need to manually clear the gradients between backward passes!\n",
    "    # Options:\n",
    "    # w1.grad *= 0\n",
    "    # w2.grad *= 0\n",
    "    # ---\n",
    "    # w1.grad[:] = 0\n",
    "    # w2.grad[:] = 0\n",
    "    # ---\n",
    "    # w1.grad.zero_()\n",
    "    # w2.grad.zero_()\n",
    "    # Best:\n",
    "    w1.grad = None\n",
    "    w2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to wrap the forward computation in torch.no_grad()!\n",
    "with torch.no_grad():\n",
    "    p = (x @ w1).relu() @ w2\n",
    "\n",
    "p_vis = p.cpu().numpy()\n",
    "y_vis = y.cpu().numpy()\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch `autograd` Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под капотом каждый примитивный оператор `autograd` - это на самом деле две функции, которые работают с тензорами. Функция `forward` вычисляет выходные тензоры из входных тензоров. Функция `backward` получает градиент выходных тензоров относительно некоторого скалярного значения и вычисляет градиент входных тензоров относительно того же скалярного значения.\n",
    "\n",
    "В PyTorch мы можем легко определить наш собственный оператор `autograd`, определив подкласс `torch.autograd.Function` и реализовав на нем статические функции `forward` и `backward`. Затем мы можем использовать наш новый оператор, вызвая функцию `.apply` на этом подкласее, передавая в неё произвольные аргументы, которые будут затем переданы в наш метод `forward`.\n",
    "\n",
    "В этом примере мы определяем нашу собственную `autograd` функцию для выполнения нелинейности `ReLU` и используем ее в реализации нашей сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a context object and one or more input\n",
    "        Tensors; we must return a single Tensor (or a tuple of Tensors)\n",
    "        containing the output(s). We can use the context object to cache\n",
    "        tensors and arbitrary python objects for use in the backward pass.\n",
    "        \"\"\"\n",
    "        # ctx.save_for_backward(x)\n",
    "        mask = x >= 0\n",
    "        ctx.save_for_backward(mask)\n",
    "        \n",
    "        output = x.clamp_min(0)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive the context object and Tensors containing\n",
    "        the gradient of the loss with respect to each of the outputs produced during\n",
    "        the forward pass. We can retrieve cached data from the context object, and\n",
    "        must compute and return the gradient of the loss with respect to the inputs\n",
    "        originally passed to the forward function (or None, if no gradient wrt that\n",
    "        input exists).\n",
    "        \"\"\"\n",
    "        # x, = ctx.saved_tensors\n",
    "        # mask = x >= 0        \n",
    "        mask, = ctx.saved_tensors\n",
    "\n",
    "        grad_x = mask * grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.empty((D_in, H), device=device, requires_grad=True)\n",
    "w2 = torch.empty((H, D_out), device=device, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    w1.uniform_(-scale1, scale1)\n",
    "    w2.uniform_(-scale2, scale2)\n",
    "\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    p = MyReLU.apply(x @ w1) @ w2\n",
    "\n",
    "    loss = (1 / 2) * ((p - y) ** 2).sum() / N\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "    progress.desc = f\"loss: {loss:.8f}\"\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    w1.grad = None\n",
    "    w2.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to wrap the forward computation in torch.no_grad()!\n",
    "with torch.no_grad():\n",
    "    p = MyReLU.apply(x @ w1) @ w2\n",
    "\n",
    "p_vis = p.cpu().numpy()\n",
    "y_vis = y.cpu().numpy()\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы ближе ознакомиться с `autograd` вам предлагаемся самим написать функцию активации `SoftPlus`: $\\log(1 + \\exp(x))$.\n",
    "\n",
    "\n",
    "\n",
    "![](https://courses.cv-gml.ru/storage/seminars/nn-training-numpy-pytorch/activation-softplus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка\n",
    "Производная `SoftPlus` имеет вид: $\\frac{\\exp(x)}{1 + \\exp(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPlus(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "\n",
    "        exp = torch.exp(x)          # can result in `torch.inf` for large x\n",
    "        mask = exp > 1e6\n",
    "        output = torch.log1p(exp)   # more numerically stable than `torch.log(1 + exp)`\n",
    "        output[mask] = x[mask]\n",
    "\n",
    "        ctx.save_for_backward(exp, mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        exp, mask = ctx.saved_tensors\n",
    "\n",
    "        partial = exp / (1 + exp)\n",
    "        partial[mask] = 1\n",
    "        grad_x = partial * grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте `SoftPlus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_test = torch.from_numpy(np.load(\"test/inp.npy\")).to(device)\n",
    "inp_test.requires_grad = True\n",
    "\n",
    "out_pred = SoftPlus.apply(inp_test)\n",
    "loss = out_pred.sum()\n",
    "loss.backward()\n",
    "\n",
    "out_test = np.load(\"test/out.npy\")\n",
    "inp_grad_test = np.load(\"test/inp_grad.npy\")\n",
    "assert np.allclose(\n",
    "    out_pred.detach().cpu().numpy(), out_test\n",
    "), \"Most likely, the forward pass implementation is incorrect\"\n",
    "assert np.allclose(\n",
    "    inp_test.grad.cpu().numpy(), inp_grad_test\n",
    "), \"Most likely, the backward pass implementation is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйем обучить что-нибудь с ипользованием данной функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the train loop from previous section\n",
    "+...\n",
    "\n",
    "# Use SoftPlus in the forward computation instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "+...\n",
    "\n",
    "# Use SoftPlus in the forward computation instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение вычислительного графа и `autograd` уже являются очень мощными инструментами для разработки моделей глубокого обучения, но тем не менее для полноценной работы с нейронными сетями являются слишком низкоуровневыми.\n",
    "\n",
    "В PyTorch реализован еще один уровень абстракции в виде пакета `nn`, который позволяет не задумываться про написание `autograd` функций, а использовать уже готовые, с написанными `forward` и `backward` функциями.\n",
    "\n",
    "Пакет `nn` определяет набор модулей, которые примерно эквивалентны слоям нейронной сети. Модуль получает входные тензоры и вычисляет выходные тензоры, но также может хранить внутреннее состояние, например, тензоры, содержащие обучаемые параметры. Пакет `nn` также определяет набор полезных функций потерь, которые обычно используются при обучении нейронных сетей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers.\n",
    "# nn.Sequential is a Module which contains other Modules, and applies\n",
    "# them in sequence to produce its output. nn.Linear is a Module that\n",
    "# computes applies a linear function and holds internal Tensors\n",
    "# for its weights biases. After constructing the model we use the\n",
    "# .to() method to move it (and all of its constituent parameters)\n",
    "# to the desired device.\n",
    "#\n",
    "# Note: Disable the bias in Linear layers here for consistency\n",
    "# with previous simplified, manually implemented examples.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ").to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions;\n",
    "# in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "#\n",
    "# Note: Pass reduction=\"sum\" in order to compute the *sum* of squared\n",
    "# errors rather than the mean; this is for consistency with the previous\n",
    "# simplified examples. In practice it is more common to use mean squared\n",
    "# error as a loss by using reduction=\"mean\" (which is the default).\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\").to(device)\n",
    "\n",
    "# Switch model into training mode\n",
    "model = model.train()\n",
    "\n",
    "# Copy-paste and edit the train loop from previous section\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Forward pass and Loss computation:\n",
    "    # - Module objects override the __call__\n",
    "    #     operator so you can call them like functions.\n",
    "    # - The torch.nn.*Loss functions are also just modules that\n",
    "    #     take two arguments - predicted values and ground truth.\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "\n",
    "    loss_ = loss.detach().item()\n",
    "    loss_hist.append(loss_)\n",
    "    progress.desc = f\"loss: {loss_:.8f}\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Iterate over all model parameters and update them.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    # Zero the gradients for all parameters in the model.\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the inference code from previous section\n",
    "\n",
    "# Don't forget to switch the model into evaluation mode\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого момента мы обновляли веса наших моделей вручную изменяя тензоры, содержащие обучаемые параметры. Это не проблема для простых алгоритмов оптимизации, таких как стохастический градиентный спуск, но на практике мы часто обучаем нейронные сети, используя более сложные оптимизаторы, такие как `AdaGrad`, `RMSProp`, `Adam` и т. д.\n",
    "\n",
    "Пакет `optim` в PyTorch абстрагирует алгоритмы оптимизации и предоставляет реализации часто используемых из них.\n",
    "\n",
    "В этом примере мы будем использовать пакет `nn` для определения нашей модели, как и раньше, но мы будем оптимизировать модель, используя алгоритм `Adam`, предоставляемый пакетом `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste model and loss function definition from last section\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ").to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\").to(device)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "#\n",
    "# Note: The best/recommended hyperparameters may vary for different optimizers.\n",
    "learning_rate = 1e-2\n",
    "num_steps = 5_000  # was 50_000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Copy-paste and edit the train loop from previous section\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer performs\n",
    "    # an update to all its tracked parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Use the optimizer object to zero all of the\n",
    "    # gradients for the Tensors it will update\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда вам потребуются модели, которые являются более сложными, чем последовательность существующих модулей; в этих случаях вы можете определить свои собственные модули, создав подкласс `nn.Module` и определив в нем метод `forward`, который принимает входные тензоры и создает выходные тензоры, используя другие модули или другие операции `autograd` с тензорами.\n",
    "\n",
    "В этом примере мы реализуем нашу двухуровневую сеть как подкласс `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        +...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        # Note: Do NOT manually call mod.forward(...) on Modules!\n",
    "        # Instead, you should ALWAYS call mod(...) directly.\n",
    "        +..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лайфхак: в ситуации, когда `forward` метод состоит из последовательного вызова слоев определенных в `__init__`, можно вместо `torch.nn.Module` отнаследоваться от `torch.nn.Sequential` и получить дефолтную реализацию `forward`.\n",
    "\n",
    "Данный подход сочитает в себе плюсы создания `torch.nn.Sequential` из списка/словаря и наследования от `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Sequential):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        +..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above.\n",
    "+...\n",
    "\n",
    "# Copy-paste loss function, optimizer definition and train loop from last section\n",
    "+...\n",
    "\n",
    "# Calling the step function on an Optimizer performs\n",
    "# an update to all its tracked parameters\n",
    "+...\n",
    "\n",
    "# Use the optimizer object to zero all of the\n",
    "# gradients for the Tensors it will update\n",
    "+..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "+..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Flow and Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера динамических графов и распределения весов мы реализуем очень странную модель: `fully connected` сеть с `ReLU`, которая на каждом прямом проходе выбирает случайное число от 1 до 4 и использует соотвествующее количество скрытых слоев, многократно используя одни и те же веса для скрытых слоев.\n",
    "\n",
    "Мы можем реализовать распределение веса между скрытыми слоями, просто повторно используя один и тот же модуль несколько раз при прямом проходе.\n",
    "\n",
    "Мы можем легко реализовать эту модель как подкласс `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pre_bias = torch.nn.Parameter(torch.ones(D_in))\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        +...\n",
    "\n",
    "        # Choose random depth during training\n",
    "        +...\n",
    "\n",
    "        # Use the maximum depth during evaluation\n",
    "        +...\n",
    "\n",
    "        # Repeat the middle_linear layer num_repeats times\n",
    "        +..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "+...\n",
    "\n",
    "# Copy-paste loss function, optimizer definition and train loop from last section\n",
    "+...\n",
    "\n",
    "# Calling the step function on an Optimizer performs\n",
    "# an update to all its tracked parameters\n",
    "+...\n",
    "\n",
    "# Use the optimizer object to zero all of the\n",
    "# gradients for the Tensors it will update\n",
    "+..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "+..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех примерах до данного момента времени, мы обучали нейросеть **сразу на \"всей\" микро-выборке из 100 сэмплов**. В большинстве случаев на практике, обучающие выборки содержат значительно больше 100 эталонных экземпляров. Как следствие, обучение с использованием всех данных может не помещаться в память ускорителей (VRAM).\n",
    "\n",
    "В связи с этим, на практике обычно датасет разбивается на батчи меньшего размера. При этом, у нас может появится желание формировать каждый батч по определенному принципу (например каждый раз создавать батч из слуачйно выбранных объектов выборки), автоматически предобрабатывать наши данные перед обучением и многое другое.\n",
    "\n",
    "В худшем случае, может быть что сами данные (или даже просто мета-данные) целиком не помещаются в оперативную память (RAM) и приходится организовывать динамическую загрузку данных с диска.\n",
    "\n",
    "Для таких нужд PyTorch предоставляет нам две абстракции - `Dataset` и `Dataloader`. `Dataset` - интерфейс, который позволяет энкапсулировать логику загрузки и подготовки экземпляров выборки. `DataLoader` - реализованный в PyTorch класс, который итерируется по указанному `Dataset`у и подготавливает из них батчи.\n",
    "\n",
    "Ниже представлен небольшой пример того, как можно реализовать свой `Dataset` и использовать `Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, S, D_in, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we initialize self.x and self.y\n",
    "        x - dataset samples\n",
    "        y - dataset labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._S = S\n",
    "\n",
    "        # Here we assume that our whole dataset fits into RAM,\n",
    "        # so we can just create all the random data here.\n",
    "        # If this were not the case, we would only load the\n",
    "        # metadata (and maybe the labels) in __init__.\n",
    "        +...\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns dataset length\n",
    "        \"\"\"\n",
    "        return self._S\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dataset sample and label with index idx\n",
    "        \"\"\"\n",
    "        # If `x` and/or `y` were too large to fit into RAM,\n",
    "        # you would have to load them from disk here.\n",
    "        #\n",
    "        # Instead, we just have to slice into `self.x/y`\n",
    "        # and convert them into PyTorch Tensors.\n",
    "        +..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте увеличем размер нашего синтетического датасета, чтобы в нем было 256 батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 256\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct out datataset and dataloader\n",
    "+...\n",
    "\n",
    "# Copy-paste model, loss function and optimizer definition from last section\n",
    "+...\n",
    "\n",
    "# Implement the missing parts of the per-epoch train loop below\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_epochs))\n",
    "for e in progress:\n",
    "\n",
    "    # In this case one \"epoch\" is a single pass through\n",
    "    # the entire dataset (seeing each sample once)\n",
    "    loss_hist_epoch = []\n",
    "    progress_epoch = tqdm(\n",
    "        train_loader,\n",
    "        total=len(train_loader),\n",
    "        leave=False,\n",
    "    )\n",
    "    for x_batch, y_batch in progress_epoch:\n",
    "        # Load the data to target device\n",
    "        +...\n",
    "\n",
    "        # Forward the model and compute the loss\n",
    "        p_batch = model(x_batch)\n",
    "        loss = loss_fn(p_batch, y_batch)\n",
    "\n",
    "        # Save the loss value to history and display it in the progress bar\n",
    "        loss_hist_epoch.append(loss.detach())\n",
    "        progress_epoch.desc = f\"loss: {loss.item():.8f}\"\n",
    "\n",
    "        # Run the backward pass, optimize parameters and clear gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Compute the average loss over the whole epoch\n",
    "    +...\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(epoch_loss)\n",
    "    progress.desc = f\"avg loss: {epoch_loss.item():.8f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single batch from the train dataloader\n",
    "# Note: Normally, you would evaluate the trained\n",
    "# model on a separate validation/testing dataset.\n",
    "+...\n",
    "\n",
    "# Copy-paste the inference code from previous section\n",
    "+..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
