{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "notebook_dir = \"/home/balabaevvl/nes/ai_economics/sem01\"  # notebook's dir\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "GPUs = [\n",
    "    \"GPU-e83bd31b-fcb9-b8de-f617-2d717619413b\",\n",
    "    \"GPU-5a9b7750-9f85-49a5-3aae-fe07b1b7661d\",\n",
    "    \"GPU-fe2d8dfd-06f2-a5c4-a7fd-4a5f23947005\",\n",
    "    \"GPU-0c320096-21ee-4060-8731-826ca2febfab\",\n",
    "    \"GPU-baef952c-6609-aace-3b78-e4e07788d5de\",\n",
    "    \"GPU-3979d65b-c238-4e9c-0c1c-1aa3f05c56a1\",\n",
    "    \"GPU-6c76a2c5-5375-aa06-11d4-0fddfac30e91\",\n",
    "]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPUs[2]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: NumPy, PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ –ø–æ—Å–≤–µ—â–µ–Ω –∏–∑—É—á–µ–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PyTorch –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω–æ–º —Å–µ–º–∏–Ω–∞—Ä–µ:\n",
    "* –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è NumPy\n",
    "* –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ PyTorch Tensors, –∫–∞–∫ –∑–∞–º–µ–Ω—ã NumPy ndarrays\n",
    "* –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–æ–¥—Å—á–µ—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –ø–æ–º–æ—â–∏ `torch.autograd`\n",
    "* –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `torch.nn` –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤\n",
    "* –ü–∞–∫–µ—Ç `torch.optim`, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –∞–±—Ç—Ä–∞–∫—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –µ–µ –≤–µ—Å–æ–≤\n",
    "* –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª–µ–π `Dataset`, `DataLoader` –∏–∑ `torch.utils.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O 'https://courses.cv-gml.ru/storage/seminars/nn-training-numpy-pytorch/test.zip'\n",
    "!unzip -o test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Increase these if figures appear small\n",
    "plt.rcParams[\"figure.figsize\"] = fx, fy = (14.08, 6.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_data(num_samples, num_features, num_targets, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    feature_locs = rng.normal(size=(1, num_features))\n",
    "    feature_scales = np.exp(rng.normal(size=(1, num_features)))\n",
    "    features = rng.normal(\n",
    "        loc=feature_locs,\n",
    "        scale=feature_scales,\n",
    "        size=(num_samples, num_features),\n",
    "    )\n",
    "\n",
    "    targets = []\n",
    "    for _ in range(num_targets):\n",
    "        num_deps = 1 + rng.integers(num_features)\n",
    "        dep_inds = rng.choice(num_features, num_deps)\n",
    "        deps = features[:, dep_inds]\n",
    "\n",
    "        offsets = rng.uniform(0, 2 * np.pi, size=num_deps)\n",
    "        scales = rng.normal(size=num_deps)\n",
    "\n",
    "        target = scales[None, :] * np.sin(deps + offsets[None, :])\n",
    "        targets.append(target.sum(axis=-1))\n",
    "\n",
    "    targets = np.stack(targets, axis=-1)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results():\n",
    "    # Show training loss history\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Training loss history\")\n",
    "    plt.plot(loss_hist)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "    # Visualize ground truth vs predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Ground truth vs predictions\")\n",
    "    for i in range(D_out):\n",
    "        plt.scatter(y_vis[:, i], p_vis[:, i], s=1)\n",
    "    plt.xlabel(\"y\")\n",
    "    plt.ylabel(\"p\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–∞–∑–º–∏–Ω–∫–∞ –Ω–∞ NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é PyTorch, –¥–∞–≤–∞–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ —Ä–µ–∞–ª–∏–∑—É–µ–º –ª–æ–≥–∏–∫—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º –ø—Ä–æ—Å—Ç–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –ø–æ–º–æ—â—å—é NumPy.\n",
    "\n",
    "NumPy - —ç—Ç–æ framework –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π; –æ–Ω –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–µ—Ç –æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∞—Ö, –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö. –û–¥–Ω–∞–∫–æ –º—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å NumPy –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–π –¥–≤—É—Ö—Å–ª–æ–π–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é, —Ä–µ–∞–ª–∏–∑—É—è –ø—Ä—è–º—ã–µ –∏ –æ–±—Ä–∞—Ç–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—ã —á–µ—Ä–µ–∑ —Å–µ—Ç—å —Å –ø–æ–º–æ—â—å—é NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is number of samples; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 100, 16, 100, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input and output data\n",
    "x, y = create_random_data(N, D_in, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "scale1 = 1 / np.sqrt(D_in)\n",
    "scale2 = 1 / np.sqrt(H)\n",
    "w1 = np.random.uniform(-scale1, scale1, size=(D_in, H))\n",
    "w2 = np.random.uniform(-scale2, scale2, size=(H, D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the train loop\n",
    "learning_rate = 1e-6\n",
    "num_steps = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{2}}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_2}{\\partial W^{2}}\\Big)^{\\!\\top}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "=\n",
    "a_1^{\\top}(h_2-y),\n",
    "\\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "=\n",
    "\\frac{\\partial}{\\partial h_2}\\Big(\\tfrac{1}{2}\\|h_2-y\\|_F^2\\Big)\n",
    "=\n",
    "h_2 - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{1}}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\\frac{\\partial \\mathcal{L}}{\\partial h_1}\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\n",
    "\\Bigg[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1}\n",
    "\\ \\odot\\\n",
    "\\frac{\\partial a_1}{\\partial h_1}\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\Big(\\frac{\\partial h_1}{\\partial W^{1}}\\Big)^{\\!\\top}\n",
    "\\Bigg[\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial h_2}\n",
    "\\Big(\\frac{\\partial h_2}{\\partial a_1}\\Big)^{\\!\\top}\n",
    "\\ \\odot\\\n",
    "\\frac{\\partial a_1}{\\partial h_1}\n",
    "\\Bigg]\n",
    "=\n",
    "x^{\\top}\\Big(((h_2-y)(W^{2})^{\\top})\\odot(h_1> 0)\\Big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x @ w1\n",
    "    h_relu = h.clip(0, None)\n",
    "    p = h_relu @ w2\n",
    "\n",
    "    # Compute the loss\n",
    "    delta = p - y\n",
    "    loss = (delta ** 2).sum()   # NOTE: say about mean and batch size\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(loss)\n",
    "    progress.desc = f\"loss: {loss:.8f}\"\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_p = 2 * delta\n",
    "    grad_w2 = h_relu.T @ grad_p\n",
    "\n",
    "    grad_h_relu = grad_p @ w2.T\n",
    "    grad_h = (h >= 0) * grad_h_relu\n",
    "    grad_w1 = x.T @ grad_h\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement inference/prediction\n",
    "p = (x @ w1).clip(0, None) @ w2\n",
    "\n",
    "p_vis = p\n",
    "y_vis = y\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy - –æ—Ç–ª–∏—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –Ω–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —á–∞—Å—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 50 —Ä–∞–∑ –∏–ª–∏ –±–æ–ª—å—à–µ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NumPy –Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥–ª—É–±–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–ó–¥–µ—Å—å –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—É—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é PyTorch - —Ç–µ–Ω–∑–æ—Ä—ã. –¢–µ–Ω–∑–æ—Ä—ã –≤ PyTorch - –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã N-–º–µ—Ä–Ω—ã–º –º–∞—Å—Å–∏–≤–∞–º NumPy. PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç—Ç–∏–º–∏ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏. –õ—é–±—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã, –≤–æ–∑–º–æ–∂–Ω–æ, –∑–∞—Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NumPy, —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é —Ç–µ–Ω–∑–æ—Ä–æ–≤ PyTorch. –ú–Ω–æ–≥–∏–µ (**–Ω–æ –Ω–µ –≤—Å–µ**) —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ –∏–º–µ—é—Ç —Ç–µ –∂–µ –∏–º–µ–Ω–∞ –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∫–∞–∫ –≤ NumPy.\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç N-–º–µ—Ä–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤ NumPy, —Ç–µ–Ω–∑–æ—Ä—ã PyTorch –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç `device` –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç—Ç–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞. –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–Ω–∑–æ—Ä –º–æ–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω—É–∂–Ω—ã–π –¥–µ–≤–∞–π—Å, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã `tensor.to(device)`, `tensor.cpu()` –∏–ª–∏ `tensor.cuda()`.\n",
    "\n",
    "–î–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ N-–º–µ—Ä–Ω—ã–º–∏ NumPy –º–∞—Å—Å–∏–≤–∞–º–∏ –∏ PyTorch —Ç–µ–Ω–∑–æ—Ä–∞–º–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `torch.from_numpy(ndarray)` –∏ –º–µ—Ç–æ–¥ `tensor.numpy()`. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PyTorch —Ç–µ–Ω–∑–æ—Ä–∞ –≤ NumPy –º–∞—Å—Å–∏–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã —ç—Ç–æ—Ç —Ç–µ–Ω–∑–æ—Ä –Ω–∞—Ö–æ–¥–∏–ª—Å—è –Ω–∞ CPU.\n",
    "\n",
    "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä—ã PyTorch, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ö–∞–∫ –∏ –≤ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–º –≤—ã—à–µ –ø—Ä–∏–º–µ—Ä–µ –Ω–∞ NumPy, –¥–∞–≤–∞–π—Ç–µ –≤—Ä—É—á–Ω—É—é —Ä–µ–∞–ª–∏–∑—É–µ–º –ø—Ä—è–º–æ–π –∏ –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥—ã –ø–æ —Å–µ—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using the GPU üòä\")\n",
    "# else:\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(\"Using the CPU üòû\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_random_data(N, D_in, D_out)\n",
    "\n",
    "# Load input and output data onto the device\n",
    "x = torch.from_numpy(x).to(dtype=torch.float32, device=device)\n",
    "y = torch.from_numpy(y).to(dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = torch.empty((D_in, H), device=device)\n",
    "w2 = torch.empty((H, D_out), device=device)\n",
    "w1.uniform_(-scale1, scale1)\n",
    "w2.uniform_(-scale2, scale2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the train loop from previous section\n",
    "# (you might not even need to change anything)\n",
    "\n",
    "# The following code is **exactly** the same as in the\n",
    "# previous train loop, sans comments and empty lines\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    h = x @ w1\n",
    "    h_relu = h.clip(0, None)\n",
    "    p = h_relu @ w2\n",
    "    delta = p - y\n",
    "    loss = (delta ** 2).sum()\n",
    "    loss_hist.append(loss)\n",
    "    progress.desc = f\"loss: {loss:.8f}\" # requires loss.item() implicitly (or equivalent) ‚Üí sync the GPU stream every iteration.\n",
    "    grad_p = 2 * delta\n",
    "    grad_w2 = h_relu.T @ grad_p\n",
    "    grad_h_relu = grad_p @ w2.T\n",
    "    grad_h = (h >= 0) * grad_h_relu\n",
    "    grad_w1 = x.T @ grad_h\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the inference code from previous section\n",
    "p = (x @ w1).clip(0, None) @ w2\n",
    "\n",
    "# Matplotlib and other CPU-based applications\n",
    "# may need CPU tensors (or NumPy ndarrays).\n",
    "if False:\n",
    "    # Before\n",
    "    p_vis = p\n",
    "    y_vis = y\n",
    "\n",
    "# After\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ –Ω–∞–º–∏ –±—ã–ª —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –∫–∞–∫ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ø–æ —Å–µ—Ç–∏, —Ç–∞–∫ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏. –†–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —Ä–∞–∑ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ –≥—Ä–æ–º–æ–∑–¥–∫–æ, –≤–µ–¥—å –∫–∞–∫ –º—ã –∑–Ω–∞–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–≥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ —Å–æ—Ç–µ–Ω —Å–ª–æ–µ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç—ã—Å—è—á–∏ –æ–ø–µ—Ä–∞—Ü–∏–π –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–Ω—ã—Ö –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö.\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ –≤ PyTorch —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ [–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ](https://en.wikipedia.org/wiki/Automatic_differentiation) (aka backprop). –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ `torch.autograd`, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –≤–∞—à–µ–π —Å–µ—Ç–∏ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –£–∑–ª—ã –≤ —ç—Ç–æ–º –≥—Ä–∞—Ñ–µ –±—É–¥—É—Ç —Ç–µ–Ω–∑–æ—Ä–∞–º–∏, –∞ —Ä–µ–±—Ä–∞ - —Ñ—É–Ω–∫—Ü–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç –≥—Ä–∞—Ñ –∑–∞—Ç–µ–º –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ –≤—ã—á–∏—Å–ª—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤–µ—Å—å–º–∞ –ø—Ä–æ—Å—Ç–æ–µ: –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –≤—ã—á–∏—Å–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞, —Ç–æ–≥–¥–∞ –º—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç `requires_grad = True` –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç—Ç–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ –∏–ª–∏ –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `tensor.requires_grad_()`. –õ—é–±—ã–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏—Ä—É–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ PyTorch —Å —ç—Ç–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç \"–ø–æ–º–Ω–∏—Ç—å\" —Å–≤–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –ø–æ–∑–∂–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ —á–µ—Ä–µ–∑ –Ω–∏—Ö. –ï—Å–ª–∏ `x` - —Ç–µ–Ω–∑–æ—Ä —Å `requires_grad = True`, —Ç–æ –ø–æ—Å–ª–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏, `x.grad` –±—É–¥–µ—Ç –¥—Ä—É–≥–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç `x` –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "\n",
    "–ï—Å–ª–∏ –Ω–∞–º –Ω–∞–æ–±–æ—Ä–æ—Ç –Ω–µ —Ö–æ—á–µ—Ç—Å—è —Å—Ç–æ—Ä–∏—Ç—å –≥—Ä–∞—Ñ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —á–µ—Ä–µ–∑ —Ç–µ–Ω–∑–æ—Ä —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –∞—Ç—Ä–∏–±—É—Ç–æ–º `requires_grad = True` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏), —Ç–æ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä `torch.no_grad()`, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞. –í —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ —É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä `x`, —è–≤–ª—è—é—â–∏–π—Å—è —á–∞—Å—Ç—å—é –≥—Ä–∞—Ñ–∞, —Ç–æ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ \"–æ—Ç—â–µ–ø–ª–µ–Ω–Ω—É—é\" –æ—Ç –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤—ã–ø–æ–ª–Ω–∏–≤ –æ–ø–µ—Ä–∞—Ü–∏—é `x.detach()`.\n",
    "\n",
    "–ù–∏–∂–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –æ–±—É—á–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.empty((D_in, H), device=device, requires_grad=True)\n",
    "w2 = torch.empty((H, D_out), device=device, requires_grad=True)\n",
    "with torch.no_grad():   # NOTE\n",
    "    w1.data.uniform_(-scale1, scale1)\n",
    "    w2.data.uniform_(-scale2, scale2)\n",
    "\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    p = (x @ w1).clip(0, None) @ w2 # NOTE: `.clamp_min(0)` is better, `.relu()` is best!!!\n",
    "\n",
    "    # Compute the loss. Loss must be a scalar tensor\n",
    "    loss = ((p - y) ** 2).sum()\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    # CAUTION! Don't forget that the loss tensor lives on the device\n",
    "    # and keeps a reference to the whole computation graph!\n",
    "    if False:\n",
    "        # WRONG! This will leak memory!\n",
    "        loss_hist.append(loss)\n",
    "\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # CAUTION! Autograd loss.backward() ACCUMULATES the gradients, it effectively\n",
    "    # does `w.grad += ...` for each tensor with requires_grad=True in the graph.\n",
    "    # You need to manually clear the gradients between backward passes!\n",
    "    # Options:\n",
    "    # w1.grad *= 0\n",
    "    # w2.grad *= 0\n",
    "    # ---\n",
    "    # w1.grad[:] = 0\n",
    "    # w2.grad[:] = 0\n",
    "    # ---\n",
    "    # w1.grad.zero_()\n",
    "    # w2.grad.zero_()\n",
    "    # Best:\n",
    "    w1.grad = None\n",
    "    w2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the inference code from previous section\n",
    "with torch.no_grad():\n",
    "    # Don't forget to wrap the forward computation in torch.no_grad()!\n",
    "    p = (x @ w1).clip(0, None) @ w2\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch `autograd` Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä `autograd` - —ç—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –¥–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏. –§—É–Ω–∫—Ü–∏—è `forward` –≤—ã—á–∏—Å–ª—è–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤. –§—É–Ω–∫—Ü–∏—è `backward` –ø–æ–ª—É—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–≥–æ –∂–µ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "\n",
    "–í PyTorch –º—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä `autograd`, –æ–ø—Ä–µ–¥–µ–ª–∏–≤ –ø–æ–¥–∫–ª–∞—Å—Å `torch.autograd.Function` –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ –Ω–∞ –Ω–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ `forward` –∏ `backward`. –ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à –Ω–æ–≤—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä, –≤—ã–∑–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—é `.apply` –Ω–∞ —ç—Ç–æ–º –ø–æ–¥–∫–ª–∞—Å–µ–µ, –ø–µ—Ä–µ–¥–∞–≤–∞—è –≤ –Ω–µ—ë –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞–Ω—ã –≤ –Ω–∞—à –º–µ—Ç–æ–¥ `forward`.\n",
    "\n",
    "–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—à—É —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é `autograd` —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ `ReLU` –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–µ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞—à–µ–π —Å–µ—Ç–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a context object and one or more input\n",
    "        Tensors; we must return a single Tensor (or a tuple of Tensors)\n",
    "        containing the output(s). We can use the context object to cache\n",
    "        tensors and arbitrary python objects for use in the backward pass.\n",
    "        \"\"\"\n",
    "        if False:\n",
    "            # You can save `x` here and compute the mask in backward\n",
    "            ctx.save_for_backward(x)\n",
    "\n",
    "        # Or you can precompute the boolean mask ahead of time and save a bit of\n",
    "        # memory (by only storing a boolean mask instead of the full float tensor)\n",
    "        mask = x > 0\n",
    "        ctx.save_for_backward(mask)\n",
    "        output = x.clip(0, None)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive the context object and Tensors containing\n",
    "        the gradient of the loss with respect to each of the outputs produced during\n",
    "        the forward pass. We can retrieve cached data from the context object, and\n",
    "        must compute and return the gradient of the loss with respect to the inputs\n",
    "        originally passed to the forward function (or None, if no gradient wrt that\n",
    "        input exists).\n",
    "        \"\"\"\n",
    "        if False:\n",
    "            (x,) = ctx.saved_tensors\n",
    "            mask = x > 0\n",
    "\n",
    "        (mask,) = ctx.saved_tensors\n",
    "        grad_x = mask * grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the train loop from previous section\n",
    "\n",
    "w1 = torch.empty((D_in, H), device=device, requires_grad=True)\n",
    "w2 = torch.empty((H, D_out), device=device, requires_grad=True)\n",
    "w1.data.uniform_(-scale1, scale1)\n",
    "w2.data.uniform_(-scale2, scale2)\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Use MyReLU in the forward computation instead\n",
    "    p = MyReLU.apply(x @ w1) @ w2\n",
    "\n",
    "    loss = ((p - y) ** 2).sum()\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "    w1.grad = None\n",
    "    w2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the inference code from previous section\n",
    "with torch.no_grad():\n",
    "    # Use MyReLU in the forward computation instead\n",
    "    p = MyReLU.apply(x @ w1) @ w2\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –±–ª–∏–∂–µ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å `autograd` –≤–∞–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—Å—è —Å–∞–º–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ `SoftPlus`: $\\log(1 + \\exp(x))$.\n",
    "\n",
    "\n",
    "\n",
    "![](https://courses.cv-gml.ru/storage/seminars/nn-training-numpy-pytorch/activation-softplus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å–∫–∞–∑–∫–∞\n",
    "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è `SoftPlus` –∏–º–µ–µ—Ç –≤–∏–¥: $\\frac{\\exp(x)}{1 + \\exp(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPlus(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        if False:\n",
    "            # 2) exp(x) may overflow even for relatively small x\n",
    "            exp = torch.exp(x)\n",
    "            ctx.save_for_backward(exp)\n",
    "\n",
    "            output = torch.log1p(exp)   # NOTE: more numerically stable than `torch.log(1 + exp)`\n",
    "            return output\n",
    "\n",
    "        if False:\n",
    "            # 4) This is better, but clipping changes the function\n",
    "            exp = torch.exp(x)\n",
    "            exp = exp.clip(None, 1e6)\n",
    "            ctx.save_for_backward(exp)\n",
    "\n",
    "            output = torch.log1p(exp)\n",
    "            return output\n",
    "\n",
    "        # SoftPlus(x) == x for sufficiently large x\n",
    "        exp = torch.exp(x)\n",
    "        mask = exp > 1e6\n",
    "        ctx.save_for_backward(exp, mask)\n",
    "\n",
    "        output = torch.log1p(exp)\n",
    "        output[mask] = x[mask]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if False:\n",
    "            # 1) We were asked to compute dL/dx, not doutput/dx\n",
    "            (exp,) = ctx.saved_tensors\n",
    "            grad_x = exp / (1 + exp)\n",
    "            return grad_x\n",
    "\n",
    "        if False:\n",
    "            # 3) After exp(x) overflows, inf/(1 + inf) is NaN\n",
    "            (exp,) = ctx.saved_tensors\n",
    "            partial = exp / (1 + exp)\n",
    "            grad_x = partial * grad_output\n",
    "            return grad_x\n",
    "\n",
    "        # SoftPlus(x) == x for sufficiently large x\n",
    "        (exp, mask) = ctx.saved_tensors\n",
    "        partial = exp / (1 + exp)\n",
    "        partial[mask] = 1\n",
    "        grad_x = partial * grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ `SoftPlus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_test = torch.from_numpy(np.load(\"test/inp.npy\")).to(device)\n",
    "inp_test.requires_grad = True\n",
    "\n",
    "out_pred = SoftPlus.apply(inp_test)\n",
    "loss = out_pred.sum()\n",
    "loss.backward()\n",
    "\n",
    "out_test = np.load(\"test/out.npy\")\n",
    "inp_grad_test = np.load(\"test/inp_grad.npy\")\n",
    "assert np.allclose(\n",
    "    out_pred.detach().cpu().numpy(), out_test\n",
    "), \"Most likely, the forward pass implementation is incorrect\"\n",
    "assert np.allclose(\n",
    "    inp_test.grad.cpu().numpy(), inp_grad_test\n",
    "), \"Most likely, the backward pass implementation is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–π–µ–º –æ–±—É—á–∏—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å —Å –∏–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the train loop from previous section\n",
    "w1 = torch.empty((D_in, H), device=device, requires_grad=True)\n",
    "w2 = torch.empty((H, D_out), device=device, requires_grad=True)\n",
    "w1.data.uniform_(-scale1, scale1)\n",
    "w2.data.uniform_(-scale2, scale2)\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Use SoftPlus in the forward computation instead\n",
    "    p = SoftPlus.apply(x @ w1) @ w2\n",
    "\n",
    "    loss = ((p - y) ** 2).sum()\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "    w1.grad = None\n",
    "    w2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "with torch.no_grad():\n",
    "    # Use SoftPlus in the forward computation instead\n",
    "    p = SoftPlus.apply(x @ w1) @ w2\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∏ `autograd` —É–∂–µ —è–≤–ª—è—é—Ç—Å—è –æ—á–µ–Ω—å –º–æ—â–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ —Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ —è–≤–ª—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏.\n",
    "\n",
    "–í PyTorch —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –µ—â–µ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –≤ –≤–∏–¥–µ –ø–∞–∫–µ—Ç–∞ `nn`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –∑–∞–¥—É–º—ã–≤–∞—Ç—å—Å—è –ø—Ä–æ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ `autograd` —Ñ—É–Ω–∫—Ü–∏–π, –∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–∂–µ –≥–æ—Ç–æ–≤—ã–µ, —Å –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–º–∏ `forward` –∏ `backward` —Ñ—É–Ω–∫—Ü–∏—è–º–∏.\n",
    "\n",
    "–ü–∞–∫–µ—Ç `nn` –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–±–æ—Ä –º–æ–¥—É–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ—Ä–Ω–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã —Å–ª–æ—è–º –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –ú–æ–¥—É–ª—å –ø–æ–ª—É—á–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã, –Ω–æ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ–Ω–∑–æ—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –ü–∞–∫–µ—Ç `nn` —Ç–∞–∫–∂–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–±–æ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers.\n",
    "# nn.Sequential is a Module which contains other Modules, and applies\n",
    "# them in sequence to produce its output. nn.Linear is a Module that\n",
    "# computes applies a linear function and holds internal Tensors\n",
    "# for its weights biases. After constructing the model we use the\n",
    "# .to() method to move it (and all of its constituent parameters)\n",
    "# to the desired device.\n",
    "#\n",
    "# Note: Disable the bias in Linear layers here for consistency\n",
    "# with previous simplified, manually implemented examples.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ").to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions;\n",
    "# in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "#\n",
    "# Note: Pass reduction=\"sum\" in order to compute the *sum* of squared\n",
    "# errors rather than the mean; this is for consistency with the previous\n",
    "# simplified examples. In practice it is more common to use mean squared\n",
    "# error as a loss by using reduction=\"mean\" (which is the default).\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "\n",
    "# Switch model into training mode\n",
    "model = model.train()\n",
    "\n",
    "# Copy-paste and edit the train loop from previous section\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    # Forward pass and Loss computation:\n",
    "    # - Module objects override the __call__\n",
    "    #     operator so you can call them like functions.\n",
    "    # - The torch.nn.*Loss functions are also just modules that\n",
    "    #     take two arguments - predicted values and ground truth.\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Iterate over all model parameters and update them.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    # Zero the gradients for all parameters in the model.\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste and edit the inference code from previous section\n",
    "\n",
    "# Don't forget to switch the model into evaluation mode\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –º—ã –æ–±–Ω–æ–≤–ª—è–ª–∏ –≤–µ—Å–∞ –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≤—Ä—É—á–Ω—É—é –∏–∑–º–µ–Ω—è—è —Ç–µ–Ω–∑–æ—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –≠—Ç–æ –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, –Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º—ã —á–∞—Å—Ç–æ –æ–±—É—á–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ `AdaGrad`, `RMSProp`, `Adam` –∏ —Ç. –¥.\n",
    "\n",
    "–ü–∞–∫–µ—Ç `optim` –≤ PyTorch –∞–±—Å—Ç—Ä–∞–≥–∏—Ä—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–∑ –Ω–∏—Ö.\n",
    "\n",
    "–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç `nn` –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, –Ω–æ –º—ã –±—É–¥–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –∞–ª–≥–æ—Ä–∏—Ç–º `Adam`, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–π –ø–∞–∫–µ—Ç–æ–º `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste model and loss function definition from last section\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ").to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "#\n",
    "# Note: The best/recommended hyperparameters may vary for different optimizers.\n",
    "learning_rate = 1e-4  # was 1e-6\n",
    "num_steps = 5_000  # was 50_000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Copy-paste and edit the train loop from previous section\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer performs\n",
    "    # an update to all its tracked parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # gradients for the Tensors it will update\n",
    "    # Use the optimizer object to zero all of the\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω–æ–≥–¥–∞ –≤–∞–º –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏, —á–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π; –≤ —ç—Ç–∏—Ö —Å–ª—É—á–∞—è—Ö –≤—ã –º–æ–∂–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏, —Å–æ–∑–¥–∞–≤ –ø–æ–¥–∫–ª–∞—Å—Å `nn.Module` –∏ –æ–ø—Ä–µ–¥–µ–ª–∏–≤ –≤ –Ω–µ–º –º–µ—Ç–æ–¥ `forward`, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É—è –¥—Ä—É–≥–∏–µ –º–æ–¥—É–ª–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ `autograd` —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏.\n",
    "\n",
    "–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –Ω–∞—à—É –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–µ—Ç—å –∫–∞–∫ –ø–æ–¥–∫–ª–∞—Å—Å `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        # Note: Do NOT manually call mod.forward(...) on Modules!\n",
    "        # Instead, you should ALWAYS call mod(...) directly.\n",
    "        h = self.linear1(x)\n",
    "        h_relu = self.relu(h)\n",
    "        p = self.linear2(h_relu)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–õ–∞–π—Ñ—Ö–∞–∫: –≤ —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ `forward` –º–µ—Ç–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ —Å–ª–æ–µ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤ `__init__`, –º–æ–∂–Ω–æ –≤–º–µ—Å—Ç–æ `torch.nn.Module` –æ—Ç–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è –æ—Ç `torch.nn.Sequential` –∏ –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é `forward`.\n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ—á–∏—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –ø–ª—é—Å—ã —Å–æ–∑–¥–∞–Ω–∏—è `torch.nn.Sequential` –∏–∑ —Å–ø–∏—Å–∫–∞/—Å–ª–æ–≤–∞—Ä—è –∏ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ—Ç `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Sequential):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out).to(device)\n",
    "\n",
    "# Copy-paste loss function, optimizer definition and train loop from last section\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Flow and Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞—Ñ–æ–≤ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å: `fully connected` —Å–µ—Ç—å —Å `ReLU`, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä—è–º–æ–º –ø—Ä–æ—Ö–æ–¥–µ –≤—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 4 –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Å—Ç–≤—É—é—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤, –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –≤–µ—Å–∞ –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤.\n",
    "\n",
    "–ú—ã –º–æ–∂–µ–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏, –ø—Ä–æ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –º–æ–¥—É–ª—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–∏ –ø—Ä—è–º–æ–º –ø—Ä–æ—Ö–æ–¥–µ.\n",
    "\n",
    "–ú—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –∫–∞–∫ –ø–æ–¥–∫–ª–∞—Å—Å `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pre_bias = torch.nn.Parameter(torch.ones(D_in))\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        x_shift = x + self.pre_bias[None, :]\n",
    "        h_relu = self.input_linear(x_shift)\n",
    "        h_relu = h_relu.clip(0, None)\n",
    "\n",
    "        if self.training:\n",
    "            # Choose random depth during training\n",
    "            num_repeats = np.random.randint(4)\n",
    "        else:\n",
    "            # Use the maximum depth during evaluation\n",
    "            num_repeats = 3\n",
    "\n",
    "        # Repeat the middle_linear layer num_repeats times\n",
    "        for _ in range(num_repeats):\n",
    "            h_relu = self.middle_linear(h_relu)\n",
    "            h_relu = h_relu.clip(0, None)\n",
    "\n",
    "        p = self.output_linear(h_relu)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out).to(device)\n",
    "\n",
    "# Copy-paste loss function, optimizer definition and train loop from last section\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_steps))\n",
    "for t in progress:\n",
    "    p = model(x)\n",
    "    loss = loss_fn(p, y)\n",
    "    loss_hist.append(loss.detach())\n",
    "    progress.desc = f\"loss: {loss.item():.8f}\"\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the inference code from previous section\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–æ –¥–∞–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏, –º—ã –æ–±—É—á–∞–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å **—Å—Ä–∞–∑—É –Ω–∞ \"–≤—Å–µ–π\" –º–∏–∫—Ä–æ-–≤—ã–±–æ—Ä–∫–µ –∏–∑ 100 —Å—ç–º–ø–ª–æ–≤**. –í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, –æ–±—É—á–∞—é—â–∏–µ –≤—ã–±–æ—Ä–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ 100 —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤. –ö–∞–∫ —Å–ª–µ–¥—Å—Ç–≤–∏–µ, –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –Ω–µ –ø–æ–º–µ—â–∞—Ç—å—Å—è –≤ –ø–∞–º—è—Ç—å —É—Å–∫–æ—Ä–∏—Ç–µ–ª–µ–π (VRAM).\n",
    "\n",
    "–í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –æ–±—ã—á–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –±–∞—Ç—á–∏ –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ü—Ä–∏ —ç—Ç–æ–º, —É –Ω–∞—Å –º–æ–∂–µ—Ç –ø–æ—è–≤–∏—Ç—Å—è –∂–µ–ª–∞–Ω–∏–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –±–∞—Ç—á –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É (–Ω–∞–ø—Ä–∏–º–µ—Ä –∫–∞–∂–¥—ã–π —Ä–∞–∑ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–∞—Ç—á –∏–∑ —Å–ª—É–∞—á–π–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã–±–æ—Ä–∫–∏), –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ.\n",
    "\n",
    "–í —Ö—É–¥—à–µ–º —Å–ª—É—á–∞–µ, –º–æ–∂–µ—Ç –±—ã—Ç—å —á—Ç–æ —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ (–∏–ª–∏ –¥–∞–∂–µ –ø—Ä–æ—Å—Ç–æ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ) —Ü–µ–ª–∏–∫–æ–º –Ω–µ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å (RAM) –∏ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –æ—Ä–≥–∞–Ω–∏–∑–æ–≤—ã–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å–∫–∞.\n",
    "\n",
    "–î–ª—è —Ç–∞–∫–∏—Ö –Ω—É–∂–¥ PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–º –¥–≤–µ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ - `Dataset` –∏ `Dataloader`. `Dataset` - –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç–Ω–∫–∞–ø—Å—É–ª–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤—ã–±–æ—Ä–∫–∏. `DataLoader` - —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤ PyTorch –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É `Dataset`—É –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏–∑ –Ω–∏—Ö –±–∞—Ç—á–∏.\n",
    "\n",
    "–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–µ–±–æ–ª—å—à–æ–π –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–≤–æ–π `Dataset` –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, S, D_in, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we initialize self.x and self.y\n",
    "        x - dataset samples\n",
    "        y - dataset labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._S = S\n",
    "\n",
    "        # Here we assume that our whole dataset fits into RAM,\n",
    "        # so we can just create all the random data here.\n",
    "        # If this were not the case, we would only load the\n",
    "        # metadata (and maybe the labels) in __init__.\n",
    "        x, y = create_random_data(S, D_in, D_out)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns dataset length\n",
    "        \"\"\"\n",
    "        return self._S\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dataset sample and label with index idx\n",
    "        \"\"\"\n",
    "        # If `x` and/or `y` were too large to fit into RAM,\n",
    "        # you would have to load them from disk here.\n",
    "        #\n",
    "        # Instead, we just have to slice into `self.x/y`\n",
    "        # and convert them into PyTorch Tensors.\n",
    "        x = torch.from_numpy(self.x[idx]).to(dtype=torch.float32)\n",
    "        y = torch.from_numpy(self.y[idx]).to(dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ —É–≤–µ–ª–∏—á–µ–º —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, —á—Ç–æ–±—ã –≤ –Ω–µ–º –±—ã–ª–æ 256 –±–∞—Ç—á–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 256\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct out datataset and dataloader\n",
    "dataset = RandomDataset(N * num_batches, D_in, D_out)\n",
    "train_loader = DataLoader(dataset, batch_size=N)\n",
    "\n",
    "# Copy-paste model, loss function and optimizer definition from last section\n",
    "model = DynamicNet(D_in, H, D_out).to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Implement the missing parts of the per-epoch train loop below\n",
    "model = model.train()\n",
    "loss_hist = []\n",
    "progress = tqdm(range(num_epochs))\n",
    "for e in progress:\n",
    "\n",
    "    # In this case one \"epoch\" is a single pass through\n",
    "    # the entire dataset (seeing each sample once)\n",
    "    loss_hist_epoch = []\n",
    "    progress_epoch = tqdm(\n",
    "        train_loader,\n",
    "        total=len(train_loader),\n",
    "        leave=False,\n",
    "    )\n",
    "    for x_batch, y_batch in progress_epoch:\n",
    "        # Load the data to target device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward the model and compute the loss\n",
    "        p_batch = model(x_batch)\n",
    "        loss = loss_fn(p_batch, y_batch)\n",
    "\n",
    "        # Save the loss value to history and display it in the progress bar\n",
    "        loss_hist_epoch.append(loss.detach())\n",
    "        progress_epoch.desc = f\"loss: {loss.item():.8f}\"\n",
    "\n",
    "        # Run the backward pass, optimize parameters and clear gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Compute the average loss over the whole epoch\n",
    "    epoch_loss = sum(loss_hist_epoch) / len(loss_hist_epoch)\n",
    "\n",
    "    # Save the loss value to history and display it in the progress bar\n",
    "    loss_hist.append(epoch_loss)\n",
    "    progress.desc = f\"avg loss: {epoch_loss.item():.8f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single batch from the train dataloader\n",
    "# Note: Normally, you would evaluate the trained\n",
    "# model on a separate validation/testing dataset.\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Copy-paste the inference code from previous section\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "y_vis = y.cpu()\n",
    "p_vis = p.cpu()\n",
    "loss_hist = [l.cpu() for l in loss_hist]\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
