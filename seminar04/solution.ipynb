{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a077b",
   "metadata": {},
   "source": [
    "# How Residual Block works internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A small MLP-like block: x -> Linear -> ReLU -> Linear\n",
    "    Computes H(x).\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d, d, bias=False)\n",
    "        self.fc2 = nn.Linear(d, d, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: y = x + F(x), where F is a small stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.f = PlainBlock(d)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.f(x)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers to measure gradients\n",
    "# ----------------------------\n",
    "\n",
    "def grad_norm(t: torch.Tensor) -> float:\n",
    "    if t.grad is None:\n",
    "        return float(\"nan\")\n",
    "    return float(t.grad.norm().detach().cpu())\n",
    "\n",
    "\n",
    "def weight_norm(m: nn.Module) -> float:\n",
    "    s = 0.0\n",
    "    for p in m.parameters():\n",
    "        s += float(p.detach().pow(2).sum().cpu())\n",
    "    return math.sqrt(s)\n",
    "\n",
    "\n",
    "def cosine(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    return float((a @ b).detach().cpu() / (a.norm() * b.norm() + 1e-12))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Experiment\n",
    "# ----------------------------\n",
    "\n",
    "def run_once(block: nn.Module, *, d=128, batch=32, small_init=1e-3, seed=0) -> dict:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    x = torch.randn(batch, d, requires_grad=True)\n",
    "\n",
    "    # small init\n",
    "    with torch.no_grad():\n",
    "        for p in block.parameters():\n",
    "            p.zero_()\n",
    "            p.add_(small_init * torch.randn_like(p))\n",
    "\n",
    "    saved = {}\n",
    "\n",
    "    def save_grad(name: str):\n",
    "        def hook(grad: torch.Tensor):\n",
    "            saved[name] = grad.detach()\n",
    "        return hook\n",
    "\n",
    "    # Forward pass that *uses the hooked tensor*\n",
    "    if isinstance(block, ResidualBlock):\n",
    "        # F(x) = fc2(relu(fc1(x)))\n",
    "        a1 = block.f.fc1(x)\n",
    "        a1.retain_grad()\n",
    "        a1.register_hook(save_grad(\"grad_a1\"))\n",
    "\n",
    "        h1 = F.relu(a1)\n",
    "        fx = block.f.fc2(h1)\n",
    "        out = x + fx\n",
    "    else:\n",
    "        # H(x) = fc2(relu(fc1(x)))\n",
    "        a1 = block.fc1(x)\n",
    "        a1.retain_grad()\n",
    "        a1.register_hook(save_grad(\"grad_a1\"))\n",
    "\n",
    "        h1 = F.relu(a1)\n",
    "        out = block.fc2(h1)\n",
    "\n",
    "    loss = out.pow(2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    info = {\n",
    "        \"loss\": float(loss.detach().cpu()),\n",
    "        \"x_grad_norm\": grad_norm(x),\n",
    "        \"a1_grad_norm\": float(a1.grad.norm().detach().cpu()),  # use a1.grad directly\n",
    "        \"w_norm\": weight_norm(block),\n",
    "    }\n",
    "\n",
    "    total = 0.0\n",
    "    for p in block.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += float(p.grad.detach().pow(2).sum().cpu())\n",
    "    info[\"param_grad_norm\"] = math.sqrt(total)\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 256\n",
    "batch = 64\n",
    "small_init = 1e-4  # try 1e-2, 1e-4, 1e-6 to see the difference more clearly\n",
    "seed = 123\n",
    "\n",
    "plain = PlainBlock(d)\n",
    "resid = ResidualBlock(d)\n",
    "\n",
    "plain_info = run_once(plain, d=d, batch=batch, small_init=small_init, seed=seed)\n",
    "resid_info = run_once(resid, d=d, batch=batch, small_init=small_init, seed=seed)\n",
    "\n",
    "print(\"=== Plain block H(x) ===\")\n",
    "for k, v in plain_info.items():\n",
    "    print(f\"{k:>16s}: {v:.6g}\")\n",
    "\n",
    "print(\"\\n=== Residual block x + F(x) ===\")\n",
    "for k, v in resid_info.items():\n",
    "    print(f\"{k:>16s}: {v:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5092db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "x = torch.randn(batch, d, requires_grad=True)\n",
    "\n",
    "# Re-init weights small again for a controlled comparison\n",
    "def small_init_(m: nn.Module):\n",
    "    with torch.no_grad():\n",
    "        for p in m.parameters():\n",
    "            p.zero_()\n",
    "            p.add_(small_init * torch.randn_like(p))\n",
    "\n",
    "plain2 = PlainBlock(d)\n",
    "resid2 = ResidualBlock(d)\n",
    "small_init_(plain2)\n",
    "small_init_(resid2)\n",
    "\n",
    "# Plain gradient\n",
    "y_plain = plain2(x)\n",
    "loss_plain = y_plain.pow(2).mean()\n",
    "loss_plain.backward(retain_graph=True)\n",
    "gx_plain = x.grad.detach().clone()\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "# Residual gradient\n",
    "y_resid = resid2(x)\n",
    "loss_resid = y_resid.pow(2).mean()\n",
    "loss_resid.backward()\n",
    "gx_resid = x.grad.detach().clone()\n",
    "\n",
    "print(\"\\n=== Input gradient comparison (same x) ===\")\n",
    "print(f\"||dL/dx|| plain : {gx_plain.norm().item():.6g}\")\n",
    "print(f\"||dL/dx|| resid : {gx_resid.norm().item():.6g}\")\n",
    "print(f\"cos(dL/dx_plain, dL/dx_resid): {cosine(gx_plain, gx_resid):.6g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533afc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_3_14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
