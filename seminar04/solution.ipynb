{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ffe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31769a41",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b855af",
   "metadata": {},
   "source": [
    "## Why CNNs exist\n",
    "\n",
    "Suppose you have a 224×224 RGB image:\n",
    "\n",
    "- Inputs = 224×224×3 = 150,528 numbers.\n",
    "\n",
    "- A single fully-connected layer to 1,000 hidden units would need:\n",
    "  - 150,528 × 1,000 ≈ **150 million** weights (+ biases)\n",
    "\n",
    "- This is:\n",
    "  - expensive (memory/compute),\n",
    "\n",
    "  - statistically inefficient (too many degrees of freedom),\n",
    "\n",
    "  - ignores a huge prior: **nearby pixels matter together**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad2b36",
   "metadata": {},
   "source": [
    "## What is convolution\n",
    "\n",
    "For 2D convolution on an image tensor `x` shaped `(C_in, H, W)`:\n",
    "\n",
    "- We learn `C_out` filters (kernels).\n",
    "\n",
    "- Each filter is shaped `(C_in, K, K)`, e.g. `(3, 3, 3)`.\n",
    "\n",
    "- For each output channel `o` and spatial position `(i, j)`:\n",
    "\n",
    "$$\n",
    "y[o,i,j] = b[o] + \\sum_{c=0}^{C_{in}-1} \\sum_{u=0}^{K-1} \\sum_{v=0}^{K-1}\n",
    "W[o,c,u,v]\\cdot x[c, i\\cdot s + u - p,\\; j\\cdot s + v - p]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `s` = stride\n",
    "- `p` = padding\n",
    "\n",
    "### Output size\n",
    "\n",
    "For input height `H`, kernel `K`, padding `P`, stride `S`:\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor \\frac{H + 2P - K}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "Same for width.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58021cf8",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edcfe6d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"images/convs.png\"  width=\"320\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41442bdc",
   "metadata": {},
   "source": [
    "## Stride, Padding, Dilation\n",
    "\n",
    "- **Stride**:\n",
    "    <div align=\"center\">\n",
    "    <img src=\"images/stride.png\"  width=\"480\"/>\n",
    "    </div>\n",
    "\n",
    "- **Padding**:\n",
    "    <div align=\"center\">\n",
    "    <img src=\"images/padding.png\"  width=\"480\"/>\n",
    "    </div>\n",
    "    <br>\n",
    "\n",
    "    > Using padding we can keep the dimensionality when using convolution and do many iterations (with activation function).\n",
    "\n",
    "- **Dilation**:\n",
    "    <div align=\"center\">\n",
    "    <img src=\"images/dilation.png\"  width=\"480\"/>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2e7b7",
   "metadata": {},
   "source": [
    "## What is a “convolutional block”\n",
    "\n",
    "A convolutional block is a standard mini-architecture that improves training stability and performance.\n",
    "\n",
    "### The most common block\n",
    "\n",
    "Conv $\\to$ BatchNorm $\\to$ ReLU\n",
    "\n",
    "- **Conv** learns features\n",
    "\n",
    "- **BatchNorm** stabilizes gradients and training\n",
    "\n",
    "- **ReLU** adds nonlinearity (lets networks approximate complex functions)\n",
    "\n",
    "Often you also include:\n",
    "\n",
    "- **Pooling** (downsample spatial size) or use stride in conv\n",
    "\n",
    "- **Dropout** (regularization)\n",
    "\n",
    "- Sometimes multiple convs per block\n",
    "\n",
    "### Typical patterns\n",
    "\n",
    "1. **VGG-style**: (`Conv`$\\to$ `ReLU`) $×2$ $\\to$ `MaxPool`\n",
    "\n",
    "2. **Modern**: `Conv`$\\to$`BN`$\\to$`ReLU` repeated, downsample using stride-2 conv\n",
    "\n",
    "3. **Residual blocks (ResNet)**: add skip connection to ease optimization\n",
    "\n",
    "4. **Depthwise-separable blocks (MobileNet)**: reduce compute dramatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b47cd3",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902caffb",
   "metadata": {},
   "source": [
    "## I. Implementation of Convolution via `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1012a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward_numpy(\n",
    "    x: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Educational 2D convolution forward pass (NCHW), using explicit loops.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (N, C_in, H, W)\n",
    "        w: Weights of shape (C_out, C_in, K, K)\n",
    "        b: Bias of shape (C_out,) or None\n",
    "        stride: Stride\n",
    "        padding: Zero-padding on H and W\n",
    "\n",
    "    Returns:\n",
    "        y: Output tensor of shape (N, C_out, H_out, W_out)\n",
    "    \"\"\"\n",
    "    if x.ndim != 4:\n",
    "        raise ValueError(f\"x must be NCHW (4D). Got shape {x.shape}\")\n",
    "    if w.ndim != 4:\n",
    "        raise ValueError(f\"w must be (C_out, C_in, K, K). Got shape {w.shape}\")\n",
    "\n",
    "    N, C_in, H, W = x.shape\n",
    "    C_out, C_in_w, K, K2 = w.shape\n",
    "    \n",
    "    if C_in_w != C_in:\n",
    "        raise ValueError(\"Mismatch: w C_in does not match x C_in\")\n",
    "\n",
    "    # Pad input\n",
    "    if padding > 0:\n",
    "        x = np.pad(\n",
    "            x,\n",
    "            pad_width=((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "\n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    H_out = (H_pad - K) // stride + 1\n",
    "    W_out = (W_pad - K) // stride + 1\n",
    "\n",
    "    y = np.zeros((N, C_out, H_out, W_out), dtype=x.dtype)\n",
    "\n",
    "    # Convolution\n",
    "    for n in range(N):\n",
    "        for co in range(C_out):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    h0 = i * stride\n",
    "                    w0 = j * stride\n",
    "                    patch = x[n, :, h0:h0 + K, w0:w0 + K]   # (C_in, K, K)\n",
    "                    y[n, co, i, j] = np.sum(patch * w[co, :, :, :])\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "x = rng.normal(size=(2, 3, 8, 8)).astype(np.float32)          # N=2, C=3, H=W=8\n",
    "w = rng.normal(size=(4, 3, 3, 3)).astype(np.float32)          # C_out=4, K=3\n",
    "\n",
    "y = conv2d_forward_numpy(x, w, stride=1, padding=1)       # keep size 8x8\n",
    "y = np.maximum(y, 0.0)\n",
    "print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903cd9a",
   "metadata": {},
   "source": [
    "## II. nn.Conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=32,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    bias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c4229",
   "metadata": {},
   "source": [
    "- Conv1x1 (?)\n",
    "\n",
    "- Conv3x3\n",
    "\n",
    "- Conv5x5\n",
    "\n",
    "- Conv7x7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02438cc8",
   "metadata": {},
   "source": [
    "## III. Shortly on Residual Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A small MLP-like block: x -> Linear -> ReLU -> Linear\n",
    "    Computes H(x).\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d, d, bias=False)\n",
    "        self.fc2 = nn.Linear(d, d, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: y = x + F(x), where F is a small stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.f = PlainBlock(d)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.f(x)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers to measure gradients\n",
    "# ----------------------------\n",
    "\n",
    "def grad_norm(t: torch.Tensor) -> float:\n",
    "    if t.grad is None:\n",
    "        return float(\"nan\")\n",
    "    return float(t.grad.norm().detach().cpu())\n",
    "\n",
    "\n",
    "def weight_norm(m: nn.Module) -> float:\n",
    "    s = 0.0\n",
    "    for p in m.parameters():\n",
    "        s += float(p.detach().pow(2).sum().cpu())\n",
    "    return math.sqrt(s)\n",
    "\n",
    "\n",
    "def cosine(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    return float((a @ b).detach().cpu() / (a.norm() * b.norm() + 1e-12))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Experiment\n",
    "# ----------------------------\n",
    "\n",
    "def run_once(block: nn.Module, *, d=128, batch=32, small_init=1e-3, seed=0) -> dict:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    x = torch.randn(batch, d, requires_grad=True)\n",
    "\n",
    "    # small init\n",
    "    with torch.no_grad():\n",
    "        for p in block.parameters():\n",
    "            p.zero_()\n",
    "            p.add_(small_init * torch.randn_like(p))\n",
    "\n",
    "    saved = {}\n",
    "\n",
    "    def save_grad(name: str):\n",
    "        def hook(grad: torch.Tensor):\n",
    "            saved[name] = grad.detach()\n",
    "        return hook\n",
    "\n",
    "    # Forward pass that *uses the hooked tensor*\n",
    "    if isinstance(block, ResidualBlock):\n",
    "        # F(x) = fc2(relu(fc1(x)))\n",
    "        a1 = block.f.fc1(x)\n",
    "        a1.retain_grad()\n",
    "        a1.register_hook(save_grad(\"grad_a1\"))\n",
    "\n",
    "        h1 = F.relu(a1)\n",
    "        fx = block.f.fc2(h1)\n",
    "        out = x + fx\n",
    "    else:\n",
    "        # H(x) = fc2(relu(fc1(x)))\n",
    "        a1 = block.fc1(x)\n",
    "        a1.retain_grad()\n",
    "        a1.register_hook(save_grad(\"grad_a1\"))\n",
    "\n",
    "        h1 = F.relu(a1)\n",
    "        out = block.fc2(h1)\n",
    "\n",
    "    loss = out.pow(2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    info = {\n",
    "        \"loss\": float(loss.detach().cpu()),\n",
    "        \"x_grad_norm\": grad_norm(x),\n",
    "        \"a1_grad_norm\": float(a1.grad.norm().detach().cpu()),  # use a1.grad directly\n",
    "        \"w_norm\": weight_norm(block),\n",
    "    }\n",
    "\n",
    "    total = 0.0\n",
    "    for p in block.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += float(p.grad.detach().pow(2).sum().cpu())\n",
    "    info[\"param_grad_norm\"] = math.sqrt(total)\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440034bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 256\n",
    "batch = 64\n",
    "small_init = 1e-4  # try 1e-2, 1e-4, 1e-6 to see the difference more clearly\n",
    "seed = 123\n",
    "\n",
    "plain = PlainBlock(d)\n",
    "resid = ResidualBlock(d)\n",
    "\n",
    "plain_info = run_once(plain, d=d, batch=batch, small_init=small_init, seed=seed)\n",
    "resid_info = run_once(resid, d=d, batch=batch, small_init=small_init, seed=seed)\n",
    "\n",
    "print(\"=== Plain block H(x) ===\")\n",
    "for k, v in plain_info.items():\n",
    "    print(f\"{k:>16s}: {v:.6g}\")\n",
    "\n",
    "print(\"\\n=== Residual block x + F(x) ===\")\n",
    "for k, v in resid_info.items():\n",
    "    print(f\"{k:>16s}: {v:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4e577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_3_14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
